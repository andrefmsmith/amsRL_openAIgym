{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Deep Q-Network Implementation Step by Step\n",
    "This notebook is a 'code-blog' entry describing and explaining an implementation of a Deep Q-Network (DQN) as studied in [Udacity's Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).  \n",
    "\n",
    "We will go step by step and explain how and why each class and method is defined, how they work and finally how they all fit together as a workflow for implementing DQNs.  \n",
    "\n",
    "If you are just getting started with DQNs, I suggest you start by checking out this [gitbook chapter](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/deep_q_learning.html). Once you have the basic ideas down, take the time to study Google Deepmind's [landmark paper on solving Atari videomages](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf), as well as the early precursor to it [Riedmiller 2005](http://ml.informatik.uni-freiburg.de/former/_media/publications/rieecml05.pdf).  \n",
    "\n",
    "The particular implementation we are about to study is suitable for environments with continuous observation space and discrete action space, as found in openAI gym's [Box2D](https://gym.openai.com/envs/#box2d) classes of environment. If this terminology sounds like ancient greek to you, read openAI's documentation [here](http://gym.openai.com/docs/).  \n",
    "\n",
    "An important note here is that you may come across the following error when attempting to import and generate Box2D environments from openAI gym:\n",
    "```Python\n",
    "env=gym.make('LunarLander-v2')\n",
    "AttributeError: module 'gym.envs.box2d' has no attribute 'LunarLander-v2'\n",
    "AttributeError: module 'gym.envs.box2d' has no attribute 'BipedalWalker-v2'```\n",
    "\n",
    "If so, you may want to check out [this thread](https://github.com/openai/gym/issues/1603) on getting Box2D environments running and [this blogpost](https://jakevdp.github.io/blog/2017/12/05/installing-python-packages-from-jupyter/) on using pip install from Jupyter Notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import gym\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "!{sys.executable} -m pip install gym[all]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The QNetwork class\n",
    "Create the QNetwork class, which is an FC n-layered network with i) parameters for state size and action size, ii) parameters for number of units in each layer we wish to program, and 3) a method for forward propagation.\n",
    "\n",
    "The line\n",
    "```Python\n",
    "super(QNetwork, self).__init__()```\n",
    "\n",
    "is there because we are going to use QNetwork as a subclass of the parent class Agent. We want it to have both its parent class attributes (from Agent, which we will define shortly), as well as some other attributes which are specific to agents of type QNetwork but not other types of Agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=64, fc2_units=64):\n",
    "        '''Initialise parameters and build the Q-network model.\n",
    "        Arguments\n",
    "        =========\n",
    "            state_size (int): size of state space\n",
    "            action_size (int): size of action space\n",
    "            seed (int): random seed for replicability\n",
    "            '''\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed) # Sets the seed for generating random numbers\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up replay buffer size, minibatch size, discount factor gamma, soft update factor for w tau, learning rate and how often to update the local network. Then check if we have a GPU and cuda available and if so, get it ready to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = int(1e5)\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "tau = 1e-3\n",
    "lr = 5e-4\n",
    "update_every = 4\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Agent class\n",
    "\n",
    "### Agent attributes\n",
    "To endow our Agent with its QNetwork 'brain', we set attributes qnetwork_local and qnetwork_target to be instances of the class QNetwork in the following lines:\n",
    "```Python\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)```\n",
    "Because we're using **Fixed Q Targets**, we actually set up two Q-networks: one which is the target we're trying to approximate and the other the current parameters for the approximation to the Q value function.\n",
    "\n",
    "Here, we initialise the Agent's memory attribute, which is a collection of *experiences*, each of which is a tuple of form (state, action, reward, next_state).\n",
    "```Python\n",
    "        self.memory = ReplayBuffer(action_size, buffer_size, batch_size, seed)```\n",
    "And after it, we add a time-step counter.\n",
    "### Agent methods: step\n",
    "The step function,\n",
    "```Python\n",
    "        def step(self, state, action, reward, next_state, done):```\n",
    "Takes in an experience tuple and adds it to the self.memory attribute of agent, as seen above. Then, it takes one time-step and checks if the number of time-steps after which to update (*update_every*) has been reached:\n",
    "```Python\n",
    "        self.t_step = (self.t_step + 1) % update_every```\n",
    "\n",
    "If it has, *and* enough samples have been stored in memory (threshold defined by *batch_size*), it retrieves an experience tuple at random from memory and then passes it to the .learn method.\n",
    "### Agent methods: act\n",
    "*act* is the method for selecting an epsilon-greedy action given the current policy.  \n",
    "First, we take the state and convert it from numpy to a pytorch tensor, unsqueeze it with 0(*returns a new tensor with a dimension of size one inserted at the specified position*) and send it to the same device where our QNetworks reside.\n",
    "\n",
    "We then set the local QNetwork model to eval() mode. This pytorch method turns off dropout and batch normalisation. The reason for this is that in the next line,\n",
    "```Python\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)```\n",
    "   we are accessing the QNetwork final output values, by passing in a state and computing forward propagation through the QNetwork's current values. Dropout and batch normalisation are used for improving training and generalization, so here we wish to turn them off, in order to select an action. The code that follows is boilerplate for instantiating an epsilon-greedy action.\n",
    "\n",
    "### Agent methods: learn\n",
    "Here we update value parameters given a batch of experience tuples.  \n",
    "First, we *detach* the target QNetwork from the computational graph, ensuring no gradient is backpropped along this network specifically. Then we obtain the maximum predicted Q values for next states from the target model:\n",
    "```Python\n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)```\n",
    "Then, we use the Q-learning formula to compute Q-targets (approximation of true function) for current states that we're learning from. We next obtain the expected Q values from the local model, forward propping *states* through it and gather(ing) the results (think this is similar to zip).  \n",
    "Next, we compute the loss, run backprop and update the local weights:\n",
    "```Python\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()```  \n",
    "Finally, we update the target QNetwork. Notice this is a different update from that executed on the local QNetwork. On the former, we're doing a 'real' learned update with gradient descent. On the latter we're using Fixed Q Targets through the *soft_update* function, which we'll look at next.\n",
    "```Python\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, tau)```\n",
    "### Agent methods: soft update\n",
    "Here the donkey has gotten close enough, so we move the carrot a little bit. The soft update goes by the formula\n",
    "```θ_target = τ*θ_local + (1 - τ)*θ_target```, using tau as initialised at the top. The _ after copy is for making .copy an in-place method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "        # Q-network 'brain'\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
    "        \n",
    "        # Optimizer - notice only used on local qnetwork\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=lr)\n",
    "        \n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, buffer_size, batch_size, seed)\n",
    "        \n",
    "        # Timestep counter\n",
    "        self.t_step = 0\n",
    "        \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save the current experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Learn after every 'update_every' steps\n",
    "        self.t_step = (self.t_step + 1) % update_every\n",
    "        \n",
    "        # if 'update_every' time steps have been reached (modulo division above),\n",
    "        if self.t_step == 0:\n",
    "                # and if there are enough samples in memory:\n",
    "            if len(self.memory) > batch_size:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, gamma)\n",
    "                \n",
    "    def act(self, state, eps=0.):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval() # pytorch method for setting a network to evaluation (stop dropout and batchnorm)\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "        \n",
    "        if random.random > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "    \n",
    "    def learn(self, experiences, gamma):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        \n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "        \n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, tau)\n",
    "    \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The ReplayBuffer class\n",
    "Here we define ReplayBuffer which is an object with the ability to carry experiences up to a certain capacity (configured at the top, *buffer_size*).  \n",
    "The attribute of ReplayBuffer which stores experiences is, appropriately, *self.memory*. We package experiences as named tuples in the self.experience attribute:\n",
    "```Python\n",
    "        self.experience = namedtuple(\"Experience\", field_names=['state', 'action', 'reward', 'next_state', 'done'])```\n",
    "Calling self.experience on data of the right format organises that data by returning a tuple of name 'Experience', with the fields defined and named above. \n",
    "\n",
    "### ReplayBuffer methods: add\n",
    ".add takes in arguments for\n",
    "```Python\n",
    "        def add(self, state, action, reward, next_state, done):```\n",
    "and passes them to self.experience for packaging. This experience is then appended to memory, up to the capacity *buffer_size*. Because *memory* is a deque, that means if it reaches capacity, the rule goes \"new in, oldest out\", where we can imagine elements getting added left to right. Once we reach capacity, if a new element is added (at the right), the oldest element falls off at the left end, and everything shifts one step left.\n",
    "\n",
    "### ReplayBuffer methods: sample\n",
    "From *memory* we sample a number *batch_size* of experiences at random, unpack them first into concatenated numpy arrays, converting them to pytorch and then sending them to the device (GPU) where we're doing computation.  \n",
    ".sample returns tensors each containing:\n",
    "```Python\n",
    "        return (states, actions, rewards, next_states, dones)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=['state', 'action', 'reward', 'next_state', 'done'])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory, k = self.batch_size)\n",
    "        \n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Finally, we will build a function for controlling the agent-environment interaction, calling the functions and classes we have defined above appropriately. The algorithm created in this notebook can be used with environments that have continuous observation space and discrete action space.\n",
    "\n",
    "### DQN function\n",
    "We start out by defining a list of running scores and a deque (special list-like container with a max capacity then last-in-first-out bevaiour) to keep a windowed version of 100 scores, as well as initialising epsilon at a specified value.  \n",
    "We then run a for loop for a given number of episodes, and within this we nest another for loop that defines a maximum length of timesteps for each episode. Let's break down the contents of this loop:\n",
    "```Python\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break```\n",
    "    1. agent.act computes the action values for a given input state and returns the epsilon-greedy action;\n",
    "    2. Given the choice of action, env.step triggers a timestep in the environment, returning a new state, reward and a 'done' boolean.\n",
    "    3. agent.step does a number of important things:\n",
    "        3.1. First, it add's to the agent's memory the current state, action, reward, next_state and done variables;\n",
    "        3.2. Then it updates the agent's timestep counter and checks if it's a multiple of *update_every*, the frequency of episodes over which we want the agent to do replay and offline learning;\n",
    "        3.3. If that frequency has been reached *and* there are a certain number of experiences in the agent's memory, it calls the method sample() from the ReplayBuffer class to retrieve a *batch_size* number of experiences from memory.\n",
    "        3.4. Finally it calls the Agent function *learn*, which does the following:\n",
    "            3.4.1. Unpack each experience tuple into states, actions, rewards, next_state and one;\n",
    "            3.4.2. Perform Q-learning  by obtaining the max Q value for the next state from the target model, compute the Q targets given the current state, and retrieve expected Q values given the current local model;\n",
    "            3.4.3. Calculate loss, run backprop and update weights for the Local Q Network;\n",
    "            3.4.4. Calls *soft_update* function to update Q in the Target Network, moving the target weights away from the current ones by a factor of tau.\n",
    "\n",
    "\n",
    "At the end of an episode, we append that episode's score (running tally of reward) to scores and score_window and print the average of scores_window, which gives us the running average over the last 100 episodes. If we achieve the solve score, it prints this out and saves the model checkpoint. Finally, the function returns *scores*, the episode-by-episode running tally of scores attained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn(n_episodes = 2000, max_t=1000, eps_start=1.0, eps_decay=0.995, eps_end=0.01, solve_score=200.0):\n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    eps = eps_start\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        \n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "    \n",
    "        scores_window.append(score)\n",
    "        scores.append(score)\n",
    "        eps = max(eps_end, eps_end*eps)\n",
    "\n",
    "        print('\\rEpisode {}\\tAverage Score {:.2f}'.format(i_episode, np.mean(scores_window)), end='')\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window)>=solve_score:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'gym.envs.box2d' has no attribute 'BipedalWalker'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-0b9f264cd86f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'BipedalWalker-v3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# plot the scores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gym\\envs\\registration.py\u001b[0m in \u001b[0;36mmake\u001b[1;34m(id, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gym\\envs\\registration.py\u001b[0m in \u001b[0;36mmake\u001b[1;34m(self, path, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Making new env: %s'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0mspec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[1;31m# We used to have people override _reset/_step rather than\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;31m# reset/step. Set _gym_disable_underscore_compat = True on\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gym\\envs\\registration.py\u001b[0m in \u001b[0;36mmake\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     56\u001b[0m             \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mentry_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m             \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mentry_point\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m             \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gym\\envs\\registration.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mmod_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\":\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mmod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mfn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'gym.envs.box2d' has no attribute 'BipedalWalker'"
     ]
    }
   ],
   "source": [
    "env = gym.make('BipedalWalker-v3')\n",
    "env.seed(0)\n",
    "scores = dqn()\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
