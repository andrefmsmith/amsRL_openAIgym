{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Deep Q-Network Implementation Step by Step\n",
    "\n",
    "## Introduction\n",
    "This notebook is a 'code-blog' entry describing and explaining an implementation of a Deep Q-Network (DQN) as studied in [Udacity's Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).  \n",
    "\n",
    "We will go step by step and explain how and why each class and method is defined, how they work and finally how they all fit together as a workflow for implementing DQNs.  \n",
    "\n",
    "If you are just getting started with DQNs, I suggest you start by checking out this [gitbook chapter](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/deep_q_learning.html). Once you have the basic ideas down, take the time to study Google Deepmind's [landmark paper on solving Atari videomages](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf), as well as the early precursor to it [Riedmiller 2005](http://ml.informatik.uni-freiburg.de/former/_media/publications/rieecml05.pdf).  \n",
    "\n",
    "The particular implementation we are about to study is suitable for environments with continuous observation space and discrete action space, as found in openAI gym's [Box2D](https://gym.openai.com/envs/#box2d) classes of environment. If this terminology sounds like ancient greek to you, read openAI's documentation [here](http://gym.openai.com/docs/).  \n",
    "\n",
    "## Troubleshooting Box2D environment imports\n",
    "I and many others came across the following errors when attempting to import and generate Box2D environments from openAI gym:\n",
    "```Python\n",
    "env=gym.make('LunarLander-v2')\n",
    "AttributeError: module 'gym.envs.box2d' has no attribute 'LunarLander-v2'```\n",
    "```Building wheel for box2d-py (setup.py): finished with status 'error'\n",
    "error: command 'swig.exe' failed: No such file or directory.```  \n",
    "\n",
    "If you haven't, skip ahead. If you have, keep reading and I hope this helps.  \n",
    "\n",
    "I think the **first error** is caused by not having Box2D environments installed within Jupyter Notebook, even if you have installed them in your Anaconda environment. [This blogpost](https://jakevdp.github.io/blog/2017/12/05/installing-python-packages-from-jupyter/) explains difficulties and differences between packages available to your environment vs those available to a Jupyter Notebook you deploy from that environment. The **second error** is a difficulty encountered once you get over the first one. If you succeed in running pip install from your notebook, you may run into the second set of errors if you don't have swig installed on your system, or if its location hasn't been added to System Environment variables.\n",
    "\n",
    "If you run into these issues, the fix that worked for me was the following:\n",
    "1. Use the following line to allow you to run pip installs from Jupyter ([source](https://jakevdp.github.io/blog/2017/12/05/installing-python-packages-from-jupyter/)), and perform a pip install of Box2D environments. This is the last line on the imports block of this notebook;\n",
    "```Python\n",
    "import sys\n",
    "!{sys.executable} -m pip install gym[Box2D]```\n",
    "2. Download swigwin from [this sourceforge link](http://prdownloads.sourceforge.net/swig/swigwin-4.0.1.zip), unzip it and copy the path (for example C:\\swigwin-4.0.1, see [video if helpful](https://www.youtube.com/watch?v=HDD9QqLtAws)). Then add this path to your System Environment variables as shown: <img src=\"path.png\" alt=\"Drawing\" style=\"width: 600px;\"/>  \n",
    "\n",
    "3. Check that swig installed correctly: <img src=\"cmd.png\" alt=\"Drawing\" style=\"width: 400px;\"/>  \n",
    "4. Shut down the notebook, kill current Jupyter processes running in Anaconda with 2x ```Ctrl-C```, launch Anaconda and Jupyter again;\n",
    "5. Run code block 1 (all the imports and pip install) and code block 2:\n",
    "```Python\n",
    "env = gym.make('LunarLander-v2')```\n",
    "and that should instantiate any Box2D environment successfully.\n",
    "\n",
    "That should work; if not, go over [this thread](https://github.com/openai/gym/issues/1603)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym[Box2D] in c:\\users\\andre\\anaconda3\\lib\\site-packages (0.16.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\andre\\anaconda3\\lib\\site-packages (from gym[Box2D]) (1.3.1)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from gym[Box2D]) (1.4.10)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from gym[Box2D]) (1.16.5)\n",
      "Requirement already satisfied: cloudpickle~=1.2.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from gym[Box2D]) (1.2.2)\n",
      "Requirement already satisfied: six in c:\\users\\andre\\anaconda3\\lib\\site-packages (from gym[Box2D]) (1.12.0)\n",
      "Requirement already satisfied: box2d-py~=2.3.5; extra == \"box2d\" in c:\\users\\andre\\anaconda3\\lib\\site-packages (from gym[Box2D]) (2.3.8)\n",
      "Requirement already satisfied: future in c:\\users\\andre\\anaconda3\\lib\\site-packages (from pyglet<=1.5.0,>=1.4.0->gym[Box2D]) (0.17.1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "!{sys.executable} -m pip install gym[Box2D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andre\\Anaconda3\\lib\\site-packages\\gym\\logger.py:30: UserWarning: WARN: Box bound precision lowered by casting to float32\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The QNetwork class\n",
    "Create the QNetwork class, which is an FC n-layered network with i) parameters for state size and action size, ii) parameters for number of units in each layer we wish to program, and 3) a method for forward propagation.\n",
    "\n",
    "The line\n",
    "```Python\n",
    "super(QNetwork, self).__init__()```\n",
    "\n",
    "is there because we are going to use QNetwork as a subclass of the parent class Agent. We want it to have both its parent class attributes (from Agent, which we will define shortly), as well as some other attributes which are specific to agents of type QNetwork but not other types of Agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=64, fc2_units=64):\n",
    "        '''Initialise parameters and build the Q-network model.\n",
    "        Arguments\n",
    "        =========\n",
    "            state_size (int): size of state space\n",
    "            action_size (int): size of action space\n",
    "            seed (int): random seed for replicability\n",
    "            '''\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed) # Sets the seed for generating random numbers\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up replay buffer size, minibatch size, discount factor gamma, soft update factor for w tau, learning rate and how often to update the local network. Then check if we have a GPU and cuda available and if so, get it ready to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = int(1e5)\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "tau = 1e-3\n",
    "lr = 5e-4\n",
    "update_every = 4\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Agent class\n",
    "\n",
    "### Agent attributes\n",
    "To endow our Agent with its QNetwork 'brain', we set attributes qnetwork_local and qnetwork_target to be instances of the class QNetwork in the following lines:\n",
    "```Python\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)```\n",
    "Because we're using **Fixed Q Targets**, we actually set up two Q-networks: one which is the target we're trying to approximate and the other the current parameters for the approximation to the Q value function.\n",
    "\n",
    "Here, we initialise the Agent's memory attribute, which is a collection of *experiences*, each of which is a tuple of form (state, action, reward, next_state).\n",
    "```Python\n",
    "        self.memory = ReplayBuffer(action_size, buffer_size, batch_size, seed)```\n",
    "And after it, we add a time-step counter.\n",
    "### Agent methods: step\n",
    "The step function,\n",
    "```Python\n",
    "        def step(self, state, action, reward, next_state, done):```\n",
    "Takes in an experience tuple and adds it to the self.memory attribute of agent, as seen above. Then, it takes one time-step and checks if the number of time-steps after which to update (*update_every*) has been reached:\n",
    "```Python\n",
    "        self.t_step = (self.t_step + 1) % update_every```\n",
    "\n",
    "If it has, *and* enough samples have been stored in memory (threshold defined by *batch_size*), it retrieves an experience tuple at random from memory and then passes it to the .learn method.\n",
    "### Agent methods: act\n",
    "*act* is the method for selecting an epsilon-greedy action given the current policy.  \n",
    "First, we take the state and convert it from numpy to a pytorch tensor, unsqueeze it with 0(*returns a new tensor with a dimension of size one inserted at the specified position*) and send it to the same device where our QNetworks reside.\n",
    "\n",
    "We then set the local QNetwork model to eval() mode. This pytorch method turns off dropout and batch normalisation. The reason for this is that in the next line,\n",
    "```Python\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)```\n",
    "   we are accessing the QNetwork final output values, by passing in a state and computing forward propagation through the QNetwork's current values. Dropout and batch normalisation are used for improving training and generalization, so here we wish to turn them off, in order to select an action. The code that follows is boilerplate for instantiating an epsilon-greedy action.\n",
    "\n",
    "### Agent methods: learn\n",
    "Here we update value parameters given a batch of experience tuples.  \n",
    "First, we *detach* the target QNetwork from the computational graph, ensuring no gradient is backpropped along this network specifically. Then we obtain the maximum predicted Q values for next states from the target model:\n",
    "```Python\n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)```\n",
    "Then, we use the Q-learning formula to compute Q-targets (approximation of true function) for current states that we're learning from. We next obtain the expected Q values from the local model, forward propping *states* through it and gather(ing) the results (think this is similar to zip).  \n",
    "Next, we compute the loss, run backprop and update the local weights:\n",
    "```Python\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()```  \n",
    "Finally, we update the target QNetwork. Notice this is a different update from that executed on the local QNetwork. On the former, we're doing a 'real' learned update with gradient descent. On the latter we're using Fixed Q Targets through the *soft_update* function, which we'll look at next.\n",
    "```Python\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, tau)```\n",
    "### Agent methods: soft update\n",
    "Here the donkey has gotten close enough, so we move the carrot a little bit. The soft update goes by the formula\n",
    "```θ_target = τ*θ_local + (1 - τ)*θ_target```, using tau as initialised at the top. The _ after copy is for making .copy an in-place method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "        # Q-network 'brain'\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
    "        \n",
    "        # Optimizer - notice only used on local qnetwork\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=lr)\n",
    "        \n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, buffer_size, batch_size, seed)\n",
    "        \n",
    "        # Timestep counter\n",
    "        self.t_step = 0\n",
    "        \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save the current experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Learn after every 'update_every' steps\n",
    "        self.t_step = (self.t_step + 1) % update_every\n",
    "        \n",
    "        # if 'update_every' time steps have been reached (modulo division above),\n",
    "        if self.t_step == 0:\n",
    "                # and if there are enough samples in memory:\n",
    "            if len(self.memory) > batch_size:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, gamma)\n",
    "                \n",
    "    def act(self, state, eps=0.):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval() # pytorch method for setting a network to evaluation (stop dropout and batchnorm)\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "        \n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "    \n",
    "    def learn(self, experiences, gamma):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        \n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "        \n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, tau)\n",
    "    \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The ReplayBuffer class\n",
    "Here we define ReplayBuffer which is an object with the ability to carry experiences up to a certain capacity (configured at the top, *buffer_size*).  \n",
    "The attribute of ReplayBuffer which stores experiences is, appropriately, *self.memory*. We package experiences as named tuples in the self.experience attribute:\n",
    "```Python\n",
    "        self.experience = namedtuple(\"Experience\", field_names=['state', 'action', 'reward', 'next_state', 'done'])```\n",
    "Calling self.experience on data of the right format organises that data by returning a tuple of name 'Experience', with the fields defined and named above. \n",
    "\n",
    "### ReplayBuffer methods: add\n",
    ".add takes in arguments for\n",
    "```Python\n",
    "        def add(self, state, action, reward, next_state, done):```\n",
    "and passes them to self.experience for packaging. This experience is then appended to memory, up to the capacity *buffer_size*. Because *memory* is a deque, that means if it reaches capacity, the rule goes \"new in, oldest out\", where we can imagine elements getting added left to right. Once we reach capacity, if a new element is added (at the right), the oldest element falls off at the left end, and everything shifts one step left.\n",
    "\n",
    "### ReplayBuffer methods: sample\n",
    "From *memory* we sample a number *batch_size* of experiences at random, unpack them first into concatenated numpy arrays, converting them to pytorch and then sending them to the device (GPU) where we're doing computation.  \n",
    ".sample returns tensors each containing:\n",
    "```Python\n",
    "        return (states, actions, rewards, next_states, dones)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=['state', 'action', 'reward', 'next_state', 'done'])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory, k = self.batch_size)\n",
    "        \n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Finally, we will build a function for controlling the agent-environment interaction, calling the functions and classes we have defined above appropriately. The algorithm created in this notebook can be used with environments that have continuous observation space and discrete action space.\n",
    "\n",
    "### DQN function\n",
    "We start out by defining a list of running scores and a deque (special list-like container with a max capacity then last-in-first-out bevaiour) to keep a windowed version of 100 scores, as well as initialising epsilon at a specified value.  \n",
    "We then run a for loop for a given number of episodes, and within this we nest another for loop that defines a maximum length of timesteps for each episode. Let's break down the contents of this loop:\n",
    "```Python\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break```\n",
    "    1. agent.act computes the action values for a given input state and returns the epsilon-greedy action;\n",
    "    2. Given the choice of action, env.step triggers a timestep in the environment, returning a new state, reward and a 'done' boolean.\n",
    "    3. agent.step does a number of important things:\n",
    "        3.1. First, it add's to the agent's memory the current state, action, reward, next_state and done variables;\n",
    "        3.2. Then it updates the agent's timestep counter and checks if it's a multiple of *update_every*, the frequency of episodes over which we want the agent to do replay and offline learning;\n",
    "        3.3. If that frequency has been reached *and* there are a certain number of experiences in the agent's memory, it calls the method sample() from the ReplayBuffer class to retrieve a *batch_size* number of experiences from memory.\n",
    "        3.4. Finally it calls the Agent function *learn*, which does the following:\n",
    "            3.4.1. Unpack each experience tuple into states, actions, rewards, next_state and one;\n",
    "            3.4.2. Perform Q-learning  by obtaining the max Q value for the next state from the target model, compute the Q targets given the current state, and retrieve expected Q values given the current local model;\n",
    "            3.4.3. Calculate loss, run backprop and update weights for the Local Q Network;\n",
    "            3.4.4. Calls *soft_update* function to update Q in the Target Network, moving the target weights away from the current ones by a factor of tau.\n",
    "\n",
    "\n",
    "At the end of an episode, we append that episode's score (running tally of reward) to scores and score_window and print the average of scores_window, which gives us the running average over the last 100 episodes. If we achieve the solve score, it prints this out and saves the model checkpoint. Finally, the function returns *scores*, the episode-by-episode running tally of scores attained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn(n_episodes = 2000, max_t=1000, eps_start=1.0, eps_decay=0.995, eps_end=0.01, solve_score=200.0):\n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    eps = eps_start\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        \n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "    \n",
    "        scores_window.append(score)\n",
    "        scores.append(score)\n",
    "        eps = max(eps_end, eps_end*eps)\n",
    "\n",
    "        print('\\rEpisode {}\\tAverage Score {:.2f}'.format(i_episode, np.mean(scores_window)), end='')\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window)>=solve_score:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together and in context\n",
    "Before beginning to train our agent, let's take a birds-eye schematic view of how the parts of the whole implementation fit together, and what it all means in terms of our goal of training agents in continuous observation spaces.  \n",
    "\n",
    "Often in Reinforcement Learning, problems are defined by finite Markov Decision Processes. In a model-free approach, the goal is, since we don't have access to the environment's one-step dynamics and therefore its true value function, to use a Monte Carlo or Temporal Difference approach to learn by experience. These methods allow us to reach optimal (or near-optimal) policies by over time, from experience, building a Q-table which takes the form of a lookup table specifying for each state what is the value of each action available to the agent. A policy can then be determined by given each state, searching for the action with the maximum value.  \n",
    "\n",
    "This approach works very well if our observation space is finite, or discrete. A continuous observation space - which is the case for most situations in the real world - would require us to build an infinitely large Q lookup table. Therefore, to tackle continuous observation spaces with RL we can use two approaches: i) discretize the continuous space and use the same lookup-based method, or ii) use a range of methods to approximate the value function.  \n",
    "\n",
    "**Deep Q Networks** are an approach where one uses deep learning and experience to learn a function Q'(s,a) which approximates Q(s,a) by using weight parameters. These parameters can be updated systematically towards a good solution by using gradient descent and a cost function which compares the value the network expected to receive for performing an action in a given state with the value it actually experienced.  \n",
    "\n",
    "For more background, readers are directed to this [nice summary](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/deep_q_learning.html). In this particular implementation, we use two features which have been fundamental for the success and stability of deep learning-based approaches in RL: Experience Replay and Fixed Q targets.  \n",
    "\n",
    "    - In **experience replay** we decouple experience from learning. During its interaction with the environment, due to reward contingencies the agent may learn to perform only a subset of actions which lead to it generating and experiencing a limited range of states over and over again. To avoid this, instead of learning & updating Q values online, the agent stores its experiences in memory and samples randomly from this memory to update Q values offline. This prevents the agent from learning only from highly correlated state-action-reward sequences. In fewer and snazzier words, experience replay allows us to reduce part of the RL value learning problem to supervised learning: we learn action *labels* (rewarded or not) online, but action *values* offline.  \n",
    "\n",
    "    - With **fixed Q-targets** we try to solve the problem that in deep learning-based function approximation, the target we are trying to learn (good weight parameters that approximate the true value function) is the same as the set of parameters that we are modifying in order to reach it. This can create instability and prevent convergence of values towards an optimum. Using fixed Q-targets entails having 2 neural networks, a *target network* for generating target weight values in our road towards the optimal function, and a *local network* which is actually doing the learning. Every few trials we update using classical deep learning tools the local network's weights, based on how distant they are from the target network's weights. Then we shift the goalposts slightly, edging the target network weights away towards a new goal, which takes into account rewards received by the agent during an episode.  \n",
    "    \n",
    "Context and rationale given, how does all the code we've written above fit together?  \n",
    "\n",
    "Below is a high-level schematic of the 'functional neuroanatomy' of the DQN implementation we've gone over in the notebook. We will go over this schematic at a high level to understand what's going on.    \n",
    "\n",
    "<img src=\"dqn.jpg\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Agent** has:\n",
    "\n",
    "- 2 Neural networks (local and target), which are created by the class **QNetwork**. **QNetwork** does:\n",
    "    - Creation of neural network objects of an architecture you specify in its arguments;\n",
    "    - Forward propagation using state as input, if you call network(state).\n",
    "- Memory, an object of class ReplayBuffer which is used to store experiences.  \n",
    "\n",
    "The **Agent** does:\n",
    "1. *Act*, by calling on **QNetwork** *forward* to obtain the values for each available action, then running its own code to pick an epsilon-greedy action\n",
    "2. *Step*, by calling:\n",
    "    - 2.1. Class **ReplayBuffer**'s *add* method, which takes argument *buffer size* and ReplayBuffer's *experience* attribute. The former determines how much memory capacity our agent has. The latter calls on the namedtuple method to package an episode's state, action, reward, next_state and done into a tuple called *experience*. This is appended into *Memory*, which is a container of type ReplayBuffer and an attribute of Agent, with capacity for a number *buffer size* of *experiences*.\n",
    "    - 2.2. **Class ReplayBuffer**'s *sample* method, which takes argument *batch size* to determine how many *experiences* stored in *memory* to recall and use during a learning step.\n",
    "    - 2.3. The Agent's Learn method,\n",
    "3. *Learn*, by calling:\n",
    "     - 3.1. **QNetwork**'s forward method to compute the max predicted Q values for the target model, and the expected Q values from the local model. *Learn* uses its own code to compute Q targets that the current state is trying to learn\n",
    "     - 3.2. Loss, backward and optimise PyTorch methods to perform gradient descent, backpropagation and weight updating for the local **QNetwork**\n",
    "     - 3.3. The Agent's Soft Update method,\n",
    "4. *Soft update*, which moves our fixed Q target weight parameters by quantity tau, operating on the weights of target **QNetwork**, previously excluded from gradient-based updating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: -231.16\n",
      "Episode 200\tAverage Score: -126.86\n",
      "Episode 300\tAverage Score: -96.39\n",
      "Episode 400\tAverage Score: -108.00\n",
      "Episode 500\tAverage Score: -70.62\n",
      "Episode 600\tAverage Score: -22.88\n",
      "Episode 700\tAverage Score: -14.95\n",
      "Episode 800\tAverage Score: 86.50\n",
      "Episode 899\tAverage Score 200.61\n",
      "Environment solved in 799 episodes!\tAverage Score: 200.61\n"
     ]
    }
   ],
   "source": [
    "env.seed(0)\n",
    "s = env.observation_space.shape[0]\n",
    "a = env.action_space.n\n",
    "\n",
    "agent = Agent(state_size=s, action_size=a, seed=0)\n",
    "scores = dqn(n_episodes = 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize\n",
    "Training is now completed. Hopefully the agent managed to solve the task, or at least learn something useful. In the next two codeblocks we plot the agent's score per episode and then watch our trained agent perform a few trials. Change how many episodes you wish to visualise and how many frames per episode to render by toggling the variables at the top of the last codeblock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEGCAYAAACZ0MnKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO2dd7gcVdnAf+/u7em9h4QklAABkgskoQUIHeH7BD+KCCqKIAq2T0FUuiIqKCrShE9QKQpCBISEkFCEkIIhPSGV9N7Lref7Y2b2zu7OzM7u3b27e+/7e5773JkzZ2fOzs6c97zlvEeMMSiKoihKGCL5boCiKIpSPKjQUBRFUUKjQkNRFEUJjQoNRVEUJTQqNBRFUZTQlOS7Abmke/fuZtCgQfluhqIoSlExa9asLcaYHl7HWrXQGDRoEDNnzsx3MxRFUYoKEVnld0zNU4qiKEpoVGgoiqIooVGhoSiKooRGhYaiKIoSGhUaiqIoSmhUaCiKoiihUaGhKIqihEaFhqIoSivjhVlr+OuHn+bk3Co0FEVR8szyzXuoqW/I2vlemr2Wv81anbXzuVGhoSiKkkd27Kvl9F+9zY9fmpe1c9bUNVJekpvuXYWGoihKHtmxrw6Aacu3Ze2cNQ2NlJVEs3Y+Nyo0FEVR8khtQyMAn27bx9TFm5KOv/jRGhZt2JXeOesbKYvmpntv1QkLFUVRCp19tU2+jG89N5srjh/IuUf24aj+nTDG8J3nPwZg5b3nM2XRJspLIogIY4Z0SzrXnpp6pi3bSk19A+WlKjQURVFaHXtr6mPb+2oaeGjqMp7+YBVz7zib/XVNAmXTrgN86f9mxPZX/Ow8RCTuXNc9PYv3lm6hvCTCMf0756S9ap5SFEVpIVZs2cuGnQfiyhaubzI9OaYqh217a2Pbx/90ctyxmvr4urX1jby3dEvsWJk6whVFUYqb0345ldE/i+/87351YVK93TX1vDx7LeN+MdX3XHe+soCdthMdYOOueGGUq+gpNU8pipLE8zNXc2ivDhw9IDcmDiU1Nz07O/D4Xz/8lFVb9/L5Ew7i6AGd2VtbH3d8zfb9OWmXCg1FUZL4/t/nAJbzVWniQF0Dizfszpow/efH65r1+X8v3cq/l25laM/2jOjfKe5YohDJFmqeUhRFCcltL8/not//m9Xb9rF1Tw2/mriYxkYTOz53zc5QgqCuoZEHJ3/CN5/5T1batXTTHl78aG1c2f66Rp/azUM1DUVRlJDMWbsTgE27a3j47WVMWrCRMQd3Y+zQ7uyrreczv3sPgM8c3TfwPNc+NZMpizfntK27D9SlrpQBqmkoShvhhr9+xI1ZGtm2VZwA14v/8D679ludcl2j4eXZa7ngwfd8P/fApCW8PLtJEwgSGEN6tAtsw28vPzZUW7975qGh6qVL3oSGiAwQkSkislBE5ovITXZ5VxGZJCKf2P+72OUiIg+KyFIRmSMiI/PVdkUpRl6ds54JzbShtxUO+dG/uMY1J8KLOjs89jdvLuGmZ2ezfMve2DFjTFzd30z+JKVj2+HVG09m7u1n8ZevnOB5/IIRfeLb2qt93P5nju7Lk188jvMT6mWLfGoa9cB3jTGHA6OBG0RkOHAzMNkYMwyYbO8DnAsMs/+uBf7Q8k1WFKW18enWfTFTTkOj4UBdA7X1jUxelJzSwz2X7qNPdwCwcVdNUr1VW/dl1JbHr6qmojRKh4pSThzaPVa+8t7zeemGE7njwiMQEU4/rGfs2CWj+se2q8qi/PbyYznNdTzb5M2nYYxZD6y3t3eLyEKgH3ARMM6u9idgKvADu/wpY4nwaSLSWUT62OdRFMXFY+8sp2fHci46pl/WzmmMSZqB3Bo45RdTOKRXeyZ++1RuevY/vDInvktZsnE35SURDurWDq+vH/EYeo/75VTm3XE27cvDdbH3XTyCkQd1YWjPeK3h6P6d+HiN5Uc5ZkBnjrGjth76/Ei27q2lb6cKXp1rtbeiNMJHPz4z1PWaQ0E4wkVkEHAs8CHQyxEExpj1IuKIzH6AO0H8Grss7hcWkWuxNBEGDhyY03YrSqFyz2vWhLFsCY1VW/dy6i+m8tvLj03p5C1GlmzcA5AkMADOeuAdwBrtC8lSo9EnSOkv01bx+ylLOfuI3imv37GyJElgADz3tTFJs8QBKkqj9OtcCUCvjhUAdGtXTkVpbjLbusm7I1xE2gMvAN8yxgSlcvQa4pikAmMeNcZUG2Oqe/Toka1mKkpR8Mf3VjDGNeO4sdEwdfEmDtQ1b4GfBeusV/OVOYXlE9myp4bx97/NCpc/oaWp8+jUAcpKIuw6UM/fZq1JeY6ol7qCJRw6VpQGfnZ4n44c1a8Tv77smNSNzQJ5FRoiUoolMP5ijHnRLt4oIn3s430Ax7C4Bhjg+nh/oLCe4FbOuh372ZejCUNKdrjrlQWsd+U2mr9uF198cgZ3/HN+s87rmGVM0jAtv7w6Zz1LN+3htF9O9UwrnoqGxvBfqLHRMNcOuXVT73OOyjRG/SWRzM1+7cpL+Oc3T+K4QV0zPkc65DN6SoA/AguNMfe7Dk0Arra3rwZedpVfZUdRjQZ2qj+jeYz+6WQ+9/D7oeuPvfctLn1kWg5bpGSbbfushHfNH4kXpi/DvUSql2kJrM7+tpfnsXD9Lmau3MakBRtjx/y0BLD8CWu2Nzm0P1zhvUiS3zkqy9IQGtHCvL9e5NOncSLwBWCuiDixaD8E7gWeF5FrgE+Bz9nHXgPOA5YC+4AvtWxzi4e1O/Zz0zP/4fGrq+lcVeZbb8OuA2xISHKWCq+RllK4NNqqgZctPhMKTNGg1pXpNSpCQ6Nhz4F6OlU1mXTW7dzPnz5YxZsLN7F2h5WPyUmPEiQ01u08wEk/nxLbX7De23pe3+B9V9JZBCnaDE2jpcln9NR7+A9fzvCob4AbctqoVsJDU5Yyc9V2/jlnPV8YfVDK+o2NhkgRPbRKeJwUF43NtCsVatCUW2hEIvDjl+fx1w8/ZfHd51BuL3e6317kyMttUOfT4QNs3h0fSrtuh3cCwHofT7if2cqLEh+fRiFSPC1VcsZDU5fmuwlKmtzxz/mc8+t3UtZzbPbpdGCJE9Pij4U+TbOYsXIbby9JnWajxqUpRER4+T/WrOvPPvQ+1z41E2haGW/1tuROv7Y+fH6mLXuS52OAv+BJx19STOYpFRqtkHTfa2fhFqV4ePLfK1m0YXfKek6/lU7nmEZflzM+9/AHXP3E9JT1alxJ+aIRic0jmb9uFxNt38XfA6KXgsxTiSRqHqlI5543xxHe0qjQaMUI1qhx0oKNcZk4lbaDY5ZKp3NsrikrLDv31wVqNWGoTdA0Es+3ets+np62KtTnU+FeljWIe/77SIC4pVpTUUw+DRUarRD3e/O3WWv46lMz+ev0T0PVV1oXjgBIK7TU44HIdpf2ycbdHH3HRP42M3gOwwW/fdf32KbdB/jrh03PdcTD8eJnUnJIR5iGceycNLQ7Zw23JvOlMzdGhYZSMGyyo6P8nHhK68aRFeloD8FVszPCWLzRMq1NXRI8t2LeWv/5vonrR3j1u7sOeGsHn9jXT8eEFEYrEmkyNR1IYz2LYhq4qdBohbgHRE5UlFqn2iZOR5fO7+/VgTm+gkLq3BwHt8PWvbXsTShz0pcn8rlHPgDS0zT8QmvdRCMSc2qnY54qJlRotELcL7ajsgeNNMP0A821PSv5oSGDkNuWME9lY95I4gzw+euS5xD5rYzn+Cdq65u+ayoTXph7GBWJhc+mY54q1JBmL1RotGJErIcY0rNpe6Eyozhxfvd0fr/mDjBaijlr4oWEl0/DD0douWeUb9tbG/iZMO9QxKVprN/ZOk3CKjRaOc571NyImJaKqFGyS6PJRNNILsv2SNjkW/zY32ePKyIqVUhtmEiriMun8cb8jXHHDg5Ykc/JWFsMFERqdCXbNL2QTlRGY6Nh94E6SqOR5PTJId5fFRnFiSMA0pH5wZP7mv8k7Nxfxzf+mv1lZ8PMW3FwnOZ7XI7yHfuCNY0w5iZnrkg0IkmaiV+2WielSbGgmkYrJ+pyhB91+0Qu/J3/OsZBqKZRnDSZp5oXPZVNTeOtRRtTV8oxjnnKrWnUpNAkdvtEYrlxTGRekVwVpa2ju20d30LxxYl6abB7AmexmXRRmVGYpBIGzvF0fj6vAYJTlI3HIB3fA+TGN7C/roGX/rM2LiQ3VfhtYrSWF84gLTG1SHlJhCP7dsqgpYWHCo1WxvMzV/PMdGuBQ0FijvBAk0OIrkA1jcIklW82k3kaXufMxMzlR7pCY8zP3ootApVNvvXc7Djz1HSf1Ofp4PfdLhjRl2G94lfm++3lx/LwlSObfc2WRn0arYzv/31O3L6jJvstSRkWlRmFSUOjCZxN3DQjPPw5vQYY2Rw0pJr97KVZrNiyl+F9Oza1J0sTj7btbXJ+//G9Fc0+n5/QiEbiV+c7uHu7ol02VzWNVk4kwTyVKappFCapfpdMfBpe/XEmZi4/3DLDa77GmJ+95dGm+CvXNXcUZLNk4x4O7dUhK+eCpu/21ndPZXD3dlx36hDA+p5OVNXph/XklRtPyto1WxoVGq2cphnhzUt3rTPKC5NUcwdiQiONc3o9K9n8/dM1TwE88s4ybnRN1GvuvCOHT7ft49De2RMaDgf3aM+U741jYNcqwJ4zZb+LlaVRqsqK18ijQqOV02Seau7kPpUahUiDMSzesJt/frzO83h9JtFTHmWOIMnGc+A2T4WdrzFv7S4muL5j0OJJ6bCnpp5Old6hsOlwwYg+QPK9c76fCHRrZ62i2bNjebOvl0+KV9wpoYhmKfeUyozCpKHBcLa9GJOXjdzJl5TO7+81wMi3ppFIfTpOmhRUpbGWtx9H9evEK3PWB7wnwtih3fn9FSM54/Cezb5ePlFNoxXjfjebmxpCfRqFSSpfVYNt+29ultusappZmPORLfMUkDzZ1UUXe63xdikEizM4S7xPzq7zLp4/ok/g9YoBFRpthOanEclSQ5Ss4tYKvDr22DKvzcw9lc1Bg7vNmSYurMviA1lW4t8Njh3SHSCl36M0ap0j8T45e0WUjzAlKjRaMe4Htdkht5pIpCBxaxpeo2+nLJ3oOU+hYT8/2ZAd2dASGrLk04DgEOAj+3Xi3e+fxldOPjiu/KHPj2RE/6bJejFNI+Hzpx3aA4DLjx+YncYWACo0WjEGV5x+M/MJqXWqMHF3wF6/cV3Mp9HcyX1OFFb8wTXb96XtX3C3xTnfzn11TJy/IfQ5shVyC8Hrc0cjMKBrFWXR+K7yvKP68IcrRyWdI/E29+9Sxcp7z+fIfq1jNjio0Gj1xNI/6DyNVom77/RKg9Hk00jnrP5pRLbuqWXQza/yr7nr2bqnhpN+PoW7X13oeZZ7Xl3AObaTPr5NyXVvfPY/XPv0rNApQ7Lp0yiNRvjm6UM9jzlOey9txF3khLYX0aqtGaNCoxUjuNNINO9c6tPInKmLNzHo5ldjS4xmE7d2UeMhNDLzaXiVWYXOMq1PT1vF3horF9OkBd4JCB97d4Vn5ll3mx2fxprt+4D4rLNBpLW2dwqiEaFLVZnvMWgSCm7cUWD77MSHHXwy2bYmVGi0cprSSCRGdbhNBKnReRqZ89rc9QB89On2rJ+7waVqeGsa3malIIIm97mjgZxzOutMvL9sC4NufpWte4LXpfAK6S0rsSKKwq6rHWbp1bCURsXXrxHTNOz/IvDXr5wQ23bYud8SGtmY81HoFJ3QEJFzRGSxiCwVkZvz3Z5CRoSYREiK6kjznVOZ0XxycQ/dA24vTaMu5DyNt5ds5s/TVll1PfptL0HiCKkae52JP0xdBsDctcnLrsa32UtoWF3R7hrvNb0Tqc+i6huNRPyFRkzTsPaPH9SVsUOtiCq3prHrgNXujpWtf+pbUX1DEYkCvwfOBNYAM0RkgjFmQX5bVrg4L3viKDRdH4X6NPLPy7PXsmVPLdecNDhW5u6A3UuXNh0PN0/j6iemA3Dl6INCh9w6Qir239YSykv85yFMnL+B7/7t46TyctvRHGbNCsju5L7SqFDf4JNoMEHTcN8Ht9A4zA7JHdG/c9baVagUldAAjgeWGmOWA4jIs8BFgAoNH5xHPHFklu5ATX0azedPH6xi2vKt/PqyYzP6/E3PzgaIExruTszLPNWURiTcNQbd/Con2SNpN14mJccsVVPfyKCbX2VAV2vJ0sQJ342NhkZjKIlGuPbpWXHHXp27nkMnf8L0lVZa8rBCw0urypSSSIRIJPh8EY/MCu6vecmo/hw3qCuDuvsv6dpaKDbzVD9gtWt/jV0WQ0SuFZGZIjJz8+bNLdq4fOOoyG6cTiVZaKQnBYrZp7G3JlxHlCscZ+/C9bt4abZ3jiiv3y4M7nWrvTrSMFFGiXXeW7oltj1x/gbmrd2ZNGgQJElIrd6237MdJ983haG3/otj7pzoef37Jy2Jbaf6rZznMPEa155ysFf1UEQj4ht267QnlsPNR9MQkTYhMKD4hIbXLxv3OBtjHjXGVBtjqnv06NFCzcoNDY2Gx99dzr7a1J2eMYYRtye/lM7LnqjOpysDilXTWLBuF0fc9oZvQr9CYPGG3Yy4fSJ/n7UmVP2/fLgqtu0emXtFKnkl9nM63nlrd3LWA28ze7W/g/7ap2dxwW/f4z+rdyQd81vpLnEt7bU7LGGyY19qwbg/xTrcg295DUg2xf3wvMM963euSnZMd20XHykV5AjfbQvzsqhlcit3zR6XYus9s0Sxfe01wADXfn8gp73BgbqGvI2yJ87fwN2vLuS+1xenrOvbqTuaRoO/puH+ept313iOTotV0/h4jdXZvffJlhQ184cTxjpl0SbP409PW8XvpyyN7d/6j3mx7R37amPbP35pHom8uTA5HNb5eW99aR5LNu7h4j98kLKNiUL3vaVbfE1EiUIjHfaHWFL1k427Y/6TVLTzSEGeOICKRvyFhrMc7JH9OvLt8Yfw60ubTIsRV0RVW6LYhMYMYJiIDBaRMuAyYEKuLra/toHDfvw6v5yYutMOw6QFG9m5P3i0tX1vbSye3zEpbd5TQ/Xdb/I/D/u/3F7mpn8v3RrrIBJn0LprH6hroPruSfzz43Ucd8+b3P1qsovIT2TU1jdm1SmZbZzRcFB+oVyxeMNuZq1KvYRobElen7v845fm8Ys3vJ9Bx8+RDsYY9tXW87GH9pAOM1d6f7ebnp3N9r21nsdSEUbgnPnAOxzwcPp74ZXBNnFMVBqNxH6DRPbY5ikR4abxw+jdqSJ2zJEz2cjaW0wUldAwxtQD3wDeABYCzxtj5ufqeo6q/JcPP23WeeoaGjn/wXf56lMz+aZrIRkvrnj8Q8584B2MMbGOrqaukS17api+chvzXOGMjY2G+ycuZvnmPZ5CY8LH62IaQpCmsWjDbrbsqeWH/5gLwCtz1iedy88HcsiP/sXFAcIs3ziTwEqjLf+on/3rdzxH8R8u3xq373Q+DY2GpZv2sHrbvtixVHMeMqHRQF29v+ZYURruXj3yznLfY8feNSntdgHsC6FpACzfvDfw+LCe7TnvqN78xiPoIDHLbImPptGhooSrxwzyvUbiHI62QlEJDQBjzGvGmEOMMUOMMfe0xDX9bLdh+WTjHuav2wXAGleH4MXC9Va9dTsPNAkN16jqa67ok2krtvLgW0v52b8W+foonFHVpwnXNR5fyRnlbd5dw/H3vBnnnA1K9dPcEWsucRzFpSXxL/bSTbs9ZxVf9cR0Rtz+RlbbkNinXProtDhzn0hTZM74+9/m5PumcKCugS17ahh195tZbYt1HUNNg3/nHHaCXS5I5dNwWLElWGhUlEZ56POj4tYVB/jR+Yfz/NdGx5WVRIWSqPUbuP0dc28/m6P6p84Z1cZkRvEJjZak3u4pm5uywD1Kd0Y0a3fsZ+mmPUl1u7e3VvVatXVvLJOn23a8dsf+mGN84XrLjNW7Y4WvJvCbyZ+kbJOD22m6aXcN89bsDKzv5uevL8rYJJFLnBG1O+Hc2h37GX//O9zjkTPpnSWbY3bsbOF1677zfNNcBa+1GH7y8jw+2Zj8fGSD+kbDAnsQ0xKcb69q58fd/3VkbDuMTwO8o6zalzf5L7zSfgB85eSDObhH+7iygV3bxQS3M9/i1ENSB9E4EVffOM07b1VrRYVGAI5Jp7lLS7o7DaeDOPHetxh//9tJdZ1RS12DiQmrRE0nMXw2GhFfR7ifDyVMyG1JGiadP0xdxmPv+psrMsUYw6PvLGNLGmaayQs3smyz1eE699AtNBzh9uGK1P6GRLZlIBi9Jt394z9rAXh93ga++tRMID666PmZa7j8sWlpXysMz89YzRefnBG6/pAe4UNJbzpjWFLZiBQZXt0RTmEiBcH7t3v5GyfGtqMhR/+zfjSeHh3KY0Koc1Up8+84m8evrk752ZJohJX3ns83Pb5za0aFRgDZyqTZ4BPbncjE+RvYvLvGvnZjzLSSNJvbblfcehlpz/BOXafUfvO2761NMm9ZbYw/Sc8OyWsfv79sC1MWe0cFhWHa8m389LVFXPrIB4FmwrqGxpi/55o/zeSMX70dKwcodTnCY0vgBtyEyQs38tDUpXFlHyzbysi7JvHmAksoPfnvFb6fd587KPjhuj83mRs37j7gW8/h++ccmrJOKjan6ScZd2j45UkvHtk/tu08P707VfDzi4/i4IR5DOPstSY6VzaZhOasCU5BEsQQlwZREgnXtXWzNXvnN+pUWUa78pK8+MCKhWKbEd6iZCu/jZd5ygv3bNkbn5kdi9xIFAhOuxz5Y4zx9FEEESaE1nlxTr5vSqwtbhJH0FUe4Y1XPPYhAJO/eyr7axuYvmIbJw/rzrBewSuhOayzY/yXbd7LT19byO0XHhF3fN7anXzxyRkcM6Azby7cyATXaNNqY7Ij3PkNgtYYueZP1uj/+lOHxEwXr89bb7dlDzf89SNq6hvp36WKZ6Z/yqNfGBWnmT3q0rp2BAiNiDQJ8F37U4+yh/X0vm+9O1awYVe80OnVsZyNu5IFxNTF6U169btN3duXcf24ofzstYWxZ3Jgt6rY8UV3ncvE+Rs458jeiAjnHtUnbi7RI18YxcadNXFCbGuWTJzu9+yxq6pj2pwfPTtYUVHHDmj9aUCaiwqNABqytNCLe0QejYhnWGFiJ+7upBOFRmOjYX9tA3f80wqNNR51gng/IMbezZrt+5i2fGuSwKitb6SsJJJkfw5aGMcZ+YOVCfTj284CrMlTuw7U069zJTv311FeEomLbnGPvud5JML76WsL2bKnJjYfYUmCH8DR1uLWPvDII3Tf64t4dW5y1NjGXTWxMMu1O6y29OpYEbt/Tmc0b90utu2t4fjB3WhfXsIHy5oipPw0DWMM7cpLYhP0doeYFV7uEzrcvUMZFx3blx1763huppU0oUtVmafQcIItvBjepyMLEo77hQLP/NGZAAzoUsm1T8+KmZleuH4Mm3bVEI1YgsKhQ3l8d1NeEmVgtyp27K+NXXvN9n2hfUpfPnEwT/hoe9eNGxLbPnN4LxbeeU7gOzL+8J68fMOJcavxKd6o0AggW5qG25EejQgbdyWbIZ6ZvjqpzKHRQI8O5THTVX2j4daX5saOG5Oe0Lji8Q9D1bvuzx95lj/57xV87dQhSZEuYdNVu52YF/3+33Hhk4f17sDr3zoFsOzbbodtZVmUXQfqMI3QqaqUmvoG3l8WH766fHO80KizO3dHcNfWN8bWbnCbkB6yM7Q6VJRGOFDXyPIte2JCI2Yu9AiMmDh/Aw9NXcZ5R/Xmoc+Piouo2eUjNPbWNtCurElohHnc/IRGXb3hlnMP5+0lm2NCo9JjjkIq7rtkBBf89r24MufRighxQs4h0eQ66qCunud2NLZTD+nBLz43Ilbu/DYlUaF/l6okoeVHSYDjItGRnepeiAhHq5YRChUaAWQjZ//1f57FbFdIajQirN/ZJDT21NTz/IzVcfl+EkmMsmpoNLz40dqmdjY2tmiaj2Wb97BzXx3//dD7ceVho8zKSyI8PW0V1Qd1SYq3X7RhNzv31bFsyx4+m3D+8pIox939JjX1jfz5mhO48o9Nws8x8zgpKxLb5HRMN78whxdtJ3SjsWaKe7W7X+dKlm3ey4otexk7xErg50xiXLQ+OV2HYxpatdUSSO7ubLtP+oxF63clmZRSUeojNLwEmdfEtiBe/PrYuGVJf3LBcO58xdJm3/vBaVSVlfDiR2uSVuoL6T4AYPlPz0OkSYC4aVdWEnrSHjQJqzCRTolM+MaJbNiZ3r1XLFRoBJANR/i/5sWvexwViXtYb3t5Pi98FC7nkMPJ902J239m+uoWXbh++optHO2RfC5RM/Pzm5SVRDxTXjh4nRviU2I8/l58pJZz6cSOwIl8c/wXk12pOhoaTZzgcdOtXTnLNu/l061NAQDO9/MyiTijb6+IM7/n6JI0JkUO6FrJ6m37qatv5M6LjuAnL8fPad3r4f+qLE39et94+lA276nhmemrGd7HmtPgCGC3z6x/F8tX8ZWTD04SGuKZEs4br1DYo/t35uvjhnDVmEFxgQGpKIkIK+89P3R9NyP6d2ZE/9T1lGRUaASQzYVeHEqi8ZrGog3ZiZdPNdkpm6zc6j1BMbFz/O1bSz3reY0y08XvDO6R+6db97HJ9ok4pii3SSrIpOeMeN329aB0KY4/wondzyQ0N4hjBnRh9bb9lJVEuGrMoCSh4YTrugV1GE1j9JBunDC4Gz+54IiYL+nt/z2NNdv3069zJXf8cwGXjBoQ95lvjR8Wt6yp83Nm+qtGIsL3zznMan+an0vkqS8fT4cK7dZyid7dAHKRUykiwoadTSaU9a1IRa5raOSa/5tB9/bl3P3fR8alvHaTjQ7Vb+7MKpdAO+UXTRqZMwCoDyk0HOe/22kcNIhwOu2SiLC/toGPE0JHO1RYvoBffu5oHn57mefEziDuu3gE4w/vybEDuwAw/Ydn8PDby2NaT23MDNf0mTBCo2NFKdGIxNn8B3StYkBXS7PwGsl/a/whcftZzb2Uhm/OK535KRmYqpT00GDkALI1T8NNSUTiIlrS6UAPcoUzJlIISWjrGwyTF+HFOTEAACAASURBVG3iuZmr+XxIZ3umfLIp2a8QhKNhuMNsg8YE+2osTWP26h28aJsPgzKrOr9jaTTi6acYc3A3ANqVRfndFd6LMF123ADPcrAcuRcd07R0TM+OFUmpUSDBPJUgNKb/8IzYdkVphD99+fg4H0amZFNopKVptLH0HYWCCo0AcmGeikQkLWefm7KACUd+YZHZJsh38jtX+u7pGcy2Bu/Zx14RQ16hpCcPS15xzqG+0fDUByvjJggGzTLf65qZ/J3nP8YYw/Z9/gLeERqrtu3lskebfBWVpVE6VZbG8ojVNjTGpbtwc9tnjqCLx/oPfjgj7W7tyvjLV04A4s1vlQmJ+Xp2bMrQeuUJB2XkQPZCO++2hQqNADLVNLbtrfV1AkdF0jrvj85vWlwmaJZqS2kaXkuBZpP2Fcmd5t3/dSTd25d51CZulnG/zpW+531o6rIkP0AQiWGl//XQ+2za7S9knAHG6m37YwLt4StHsfCuc/j4trNiAr+2vpEO5d6CwR1CWhoVHv3CqMA2Xj9uKF85aTD/vvl0TrR/F/ej5TwvQ3q0Y8r3xgFN/odbfBYtygSJpXZvPoWgMSvBqNAIIB1NY/PuGmau3MaiDbsYedcknp/pPe8iGklPaPyPy2ThF24JuXvZ/vH1sXzpxEGx/dqA7KiJ3HfxiLQnS1Uf1IUnv3hczJwDVhoKLxNIr47lvHD92Ni+MfAD26GabTLJ5HvOkb1j246mUddgaFfu7WsoiUis4512yxmcdURv3vzOKbxhz1tJpH15CT+6YHjcZEi3ecpJ4zGkR3sG28J17u1nM/f2swIzE6RLVl0aLtFz5vBewXVVwOQFFRoBpDMj/MLfvcclD38Qy0z6zpItcctyOkQikpYwcjv7ygPNU83nsuMGJGXs7Nu5MhaKCfCZEX3jjv/ss0f5nu+zI/txz3/FHw8yIQFcetwATjusJ89cO5r+XSzNoV/nSk+n548vGE6nyqZRe3lphOvHDeEuV9bUXOI0qVfH5JxbibOfzzrC6gBH9O/kGZbbvX15XFSZsz20ZwcO7R0u5QrECw3nOu7HrX15SVzkUzaIrWCXhXMlriKpFB4qNAJIzPEUhBMF5dSdvnJb3LKcDiVpahruxGtBq89lYznWK0cfxNVjB8WVRURi121XFk3q8I4b1MX3fNGI0LGyqfO866IjPBfFcThpaHcOceWkumSUFUjft3NlLLzyH18fy9G29nLBiL5xYZcd7c6wqjT9mdB+7UnkW+ObMpo6PgO3g9ohmjBb+fTDerHk7nM9Hc+XHz+AmT8aD8BDV4zk5GHd44RhOsQJDY+U67kgVz6NfKy2qKRGf5UAnM49k+gQv3UBIpK5plEakDYhG91CRCQpxj0akZht3KvZ0YDpwCISN6q98Jh+cWGgj1/VlH765xcfxZ++fHzc5286YxhL7j6XitJozJzSsbKU5742hgV3nh2rd+XogfYxq+2ZpM9w89WTB/O/Zx/Krecfzsp7z48THpcfP5DzjrLMTk6n5l64x8FLM3J3gu/872mxbffzNXZod56+5oSMzUdu5dhpQ1BixmyQjXk3Dg9efiyXHz+Aez97FL+93H+AoeQPFRoBOGlE0lnO0Zkd6ze6szSNxtCdgnskHTTyCkrzHZZoRKgojfL2/45rKhO30Ei+RklE4nweiRFeXapK+f45hzL1e+PoVFkai4Qa0qMd44f3YtFd5/D368bwP9UDku6JuLSc8+zEd50rS6kojcZl1B13iJW6e7TtB2mu0Lj1/OHccNpQDrfNcu6IrqqyKL++9Fhm3Do+Jqi7ViULjStHHxR4Dcf0BsnzHpqDW0AMsv0YY4d086ueFbKpaQzp0Z6ffXYElx0/kF6uaC8v1KWRH1RoBOCs3JdObh2HvT6aRud2pTQ0BofP+lFW4t8ZZhIe/PkTBnLjGcPoZo+UnSa5O+9IpEnD8ZKD0Yhw22ea0pU/clV8xI+I8PVxQ2MdmIjwj6+PjTmwK0qjVA/qmnK0+r2zDmXGreNj6x+4GT+8FwvuPJsR/a2Ec34hrQCH9Ipfte2H5x0WaGIDqHKdr6qshLKSCD06lMcEdXnCmtolEfFcjMiNezDQw2Mdkkxxf/dDenXg/ZtP56snH5y183uRTU3Dj2U/PS/n11DCoUIjgFpb00hnQZZUSfuskNvGQFOTH0GfySQ8+Ii+nfjOmYfEXnrHTOIWGtFI02jfT9NwE+Ssdzh2YBc6e4zOg4hGJLBzdWseIwd24YTByZlWR/TvxMRvnxpX1rVdecq2HNm3yQ/hvjfO3RicsLhQ56rSUB3pF0YfxAOXHp2yXjqc64rYikaEvp0rc96pO7ckl9dx3/d0EzEq2UXTiARQ67GATyoS04UnYrC0gnScfCLWKN8vLTZklnuqwh4hO++j82K6zXERkZhW5CU0EvP/pLNEbK6IRoT/+9Lx/PAfc9l9oD6W6LDaI2V3RODezx7F4b078OBbSz0F8/kj+rBy66HJ62LYt6Nf50rm3n4WBhhx+0Q+f0KwacohF1Fe8RFYWT+9J1lNIxLA+MN78ebCjVmJ0lIyR4VGALGlQtPQCrwWWHJjjKUVpCOIIiI0mODP/N/7K0Ofz8ERXJEUmkaQIzxR0ygAmQFYfo0HLj0GgEE3vwrAzecmz+GIiNCtfTnfPvMQ9tc18Jmj+ybVAbghIRQZmjSN0pJIzOG/4M6zk2Zi54uW6sxbSji5/UCg8zTyRYG84oVJoqbx/rItcbmiNuw8wOcefj8uHcU+H1+GiKUpGAz1DcECoHeCA9Dpl7O9brETzuu89I7WECc0XI5wLxKd10HRVPnirv86kie+WO2p3YnLtHLr+cNjfpEwOPNXSl3fuaqspEVs/GFIJ4CjObSUcHIolPvbVlFNIwBHaEREMMZwxWMfcmivDrzxbWuG7mPvLmfGyu2xhHbgr2lERGJmpkZjqAjoiC8Z1T8uj5MVkWWyPoJ1NCjnpXciviJxjnChzCMxnkNJgpBoqY4qHb4QEMnUnA7osauqWbhhV7OjtXJFa9M0Ytez/7dUvjUlnsIbFhYQjnnKYGKmmcUbm7KrOumwO1c2OVL95mdERRAs4VOfpnnKeSkz7ZwuHum92kyiluCo+4kmp/Q0jcITGkE0p7mdqkpjYb6FSEspfdmcEa4UPio0AqixNQ3HD5HIzv2Wqco963mfj6ZhLXHZdK4gR3jiyM15KSsy1DSOGehtcnGEgRMy2rQWdHihkezTKK6uo6VNKy1JS3035yfXcX/bIC9CQ0R+ISKLRGSOiPxDRDq7jt0iIktFZLGInO0qP8cuWyoiN7dEO52FbRyTktWOpuNJ0TTAgYCZ4JaRyVrcKZ15Gs5LmWmoodfsZHf5H68+jutOHcKArpajMVFIJO47abghOXqqGISGu4lF0NyMaTnzVMvexCo74WOhBBy0NfLl05gE3GKMqReRnwO3AD8QkeHAZcARQF/gTRFxpsv+HjgTWAPMEJEJxpgFuWyke+2FmNDwqOdeRc4v5DYi1svlaBolaURkOR1xpo5wX6Fhn29w93ZxkUWJHX+igDsxID16MQgNcVQ+ay+vbcklLW2eaim+fOJg6hsNX3RlIlBajrxoGsaYicYYZ8GCaYBjdL8IeNYYU2OMWQEsBY63/5YaY5YbY2qBZ+26OcXxadQ2NHLAXrXNPapyXhb3hD5/oeFoGql9Gs4VTj/MSo/hdO5h++O/XTcmbt/vWmFDib1WiPOjEB3hiaimke3rtMhlYoEaZSURbjhtKOUBGRKU3FEI0VNfBp6zt/thCRGHNXYZwOqE8hPwQESuBa4FGDjQf5W5MDiaxubdNYy8a5J1ftdxZ1T92tz1sTI/R7iIlVrk9XkbaDQmpXnKvTZz1CMUNojjBsVPYvPTasKez0voDO7eznNCYWJ210KkyVDYun0aLSXApRVra0oyORMaIvIm0Nvj0K3GmJftOrcC9cBfnI951Dd4a0SefjdjzKPAowDV1dXN8s25zVMO7k7G6XTfXLgpVuYbcmunRHdSqKdjnkqcsR2Ee1EiBz/zVFhzl9fn//H1sbHv4qYYNA13EwtwWknWaKmfIjbXpWUup+SZnAkNY8z4oOMicjVwAXCGaUoJuwYY4KrWH1hnb/uV54xarzxScaaN5NfEb3JfYt100n44gVthHH9e8iFxLkVTebjX3MvR2bmqzDNnU3H4NNzbhd/edOlQXsLumvoW+26JwRC5IjFHmpIfQgsNETkJGGaMeVJEegDtbb9D2ojIOcAPgFONMftchyYAfxWR+7Ec4cOA6Vhd9TARGQysxXKWX5HJtdPBK9eS+3H1eleCfBpu1m7fH74dttQIJzSSG+VnMkrHsf7FsYM44/Cevscd33JRCA3Xr1j4rU2fCd88iRkrtrXY9VrqJ//2+EOoqW/kf6oHpK6s5IxQQkNEbgOqgUOBJ4FS4M/AiRle93dAOTDJHj1MM8ZcZ4yZLyLPAwuwzFY3GGMa7DZ8A3gDiAJPGGPmZ3jt0HjltnH3yV4dpP+M8Pj96kFdeXvJZu8LJ3T8zhoJFSFCbj2Fhs/ILB0T2e0XHhF43PESRCNCzw7ltK8oBHeZN/GO8NYnNgZ3b5eUeTeXtNQ97FRVGri8sNIyhH2z/xs4FvgIwBizTkTCL1ycgDEmOftb07F7gHs8yl8DXsv0mumy+0Cd5zwM9yjV62UJWrHPzU8+M5wzfvV2qLY4EwsrSqJ858xDuH/SEt+6Xu+v3+g/m1qBE8YajQjTbw20TOYdrwg4JXP0DrYtwtonam2/gwEQkZYbxuSJE+99i/nrdiWVx9vDkz8XNE/D/TmvNOfVB3kvBhQzT5VFuXiUd0qQpuskN8qvXyzNohfYuURYP0k+SWViVNLEcYTn4F6+9d1TmfCNTA0aSi4Iq2k8LyKPAJ1F5KtYYbKP5a5Z+WfXgXrPcvd74ZUq3G8tJPfotiQinp2741BMPOKYp0L5NDzkgN9oOh3zVMrrimWgKoaBe2t3hLc0uQy5PbhH+9SVlBYllNAwxvxSRM4EdmH5NX5ijJmU05YVKO5OJp11uePCPMVbaPiN0u1VZ6ksjVLTELxeRzrnzWaq9Ue+MIrH3l2e0TK2LU0+FipqC+j6Fm2DlEJDRKLAG3YIbZsUFG7iNY3gt2TSt0/hnx+v48G3lsaVRyPiqRH4+RgcTaO8NBJbt9wPr1P4nTebpqTTDuvJaYf5R1cVEq3dEd7S6C1sW6QcFtrRS/tEpFOqum0CgZ++tpALfvsuDSlGVuUl0Vg227ioqzQ1jfsuHkH/LpWUl0RCOK+t42cN79V0vQwc4U7ywtZIJM4RnseGtBJUw2hbhPVpHADmisgkIDYrzRhzY05alWdMwFuw+0A9j76zPLY/cmBnxg7pHrdokoOVDl3sczaVn3tUb0+h4SwZmnjo4lH9Yw7wVBOpnHxSj15VHVvm1Es4PHvt6EB7/uTvjEupSRUr6tNQlMwJKzRetf/aBDUe6UP8iEbEVz2PRsSzw779wiM8U5R0qixNfb0UnZzX9bzKUnWVQet9FDvq08guzrNyRD81RrQFwjrC/yQiZYCTpnyxMSZ5EkMrYW+Nd+SUFyL+sSPRiDQtUOMatJdGI9R7ONHdizn5kco85ZUyxMvs1ZZH2PEht233PmSLTpWl/O26MRzWO+OpW0oREWo4KSLjgE+w1rR4CFgiIqfksF15pV15CeMDUma4iYh/Byzi3Sn5RU+FSfaXqpPz1jRar9aQCfGRbPlrR2viuEFdY+ZVpXUTtjf5FXCWMeZUY8wpwNnAA7lrVn6pKI1y/og+oeoGmqdEYqq7O+opIsGdVVDce2pNI5wwassD7HhHeBu+EYqSAWGFRqkxZrGzY4xZgpV/qtUStjOxFlfyj05y5i241xiXBE3jox+fyZTvjQt5veSyn1wwvOmaHhP2nLJTD+nR1IZQV2ud9OvcFBmmMkNR0iOs0JgpIn8UkXH232PArFw2rFiIiL+mISKxCXSJPgy30OjarozB3dt5LxDicc5EvnzS4Ni2n6ax/Kfn8X9fOi7EFVo/j3xhVGxbFxBSlPQIKzSuB+YDNwI3YWWhvS5XjSoEwmsa/qamaMRlnmpIFBrNap4vXo5wazKhaNSQTbf25QzsWgW07kWYFCUXhA25LQF+Y4y5H2KzxMtz1qoCIKzQiCZ0xnHH4jSN+BDboPM3p0P30jS8Jw22YanhQn0aipIeYcdZkwH3FOFK4M3sN6dwyEZf4s5mm6hpiFgLG71w/ZhYmeNzOHFo94yv6TX5r6VWVismnN9Xb42ipEdYTaPCGLPH2THG7BGRqhy1qSAI25kY4z9ajTNPNSYKDUla2Kh6UFdW3nt++o1Ngfc8jaxfpihpy/NVFCUTwmoae0VkpLMjItVA+PVKi5CwnYnBvwOOuMxT+aQYlmDNF3pnFCU9wmoa3wL+JiLrsPrJvsClOWtVARC2MzHGJNX9zWXHcFC3dnGaRj7JJI1Ia8f5/qppKEp6BPZoInKciPQ2xswADgOew1q7+3VgRQu0L2+EdZAaj7odK0s5ZkBnoCmBYD7xntyX/3YpilJ8pBoGPwLU2ttjgB9ipRLZDjyaw3blnbChmMYkm6fcQsRxhLcrS73qXq5QR7iiKNkilXkqaozZZm9fCjxqjHkBeEFEZue2afkl7KQvrwl5Xov8dK4qY29t4biBVIxYBKXBVxQlmVTj6aiIOILlDOAt17Gw/pCiJFF7qPLRFIwxSeYpt8BxJts55qpCQa1TiqJkQiqh8Qzwtoi8jBUt9S6AiAwFdua4bXkl0eb/g3MOC6gbv+/WNAZ2q+KJL1bzi8+NyGbzlGaiPh1FyYxAbcEYc4+ITAb6ABNNky4fAb6Z68blk0Q3gF8UVKNH9FRiwemH9UqskXc055KiKJmQ0sRkjJnmUbYkN80pHJJNTt4Yk+xoLobUFEXQREVRCpC8TiIQke+JiBGR7va+iMiDIrJUROYkTCi8WkQ+sf+uznnbktrqXc+Y5LotJTS+dsrBfHHsIAAGdK0Mrqx4om5wRUmPvDmzRWQAcCbwqav4XGCY/XcC8AfgBBHpCtwGVGO957NEZIIxZnsO2xe/76NrGJJjbltqFH/LeYfHtid/ZxyNCZFAE75xIp9s3JP4MQUY0b8TK7bspX15q47nUJSsk8835gHg+8DLrrKLgKds38k0EeksIn2AccAkJ/xXRCYB52A56nNC0tQGH0HQrqwkqW4+pkV4+VxG9O/MiP7eUVtt3Tz184tH8KUTB9OrY0W+m6IoRUVezFMiciGw1hjzccKhfsBq1/4au8yv3Ovc14rITBGZuXnz5ua0MX7fp16XdmUeWkgb75GLgIrSaMGFQStKMZAzTUNE3gR6exy6FWtm+VleH/MoMwHlyYXGPIo9W726ujpjk3Wy9uAtCLq2KwsMuS1UNHpKUZRMyJnQMMaM9yoXkaOAwcDH9mi+P/CRiByPpUEMcFXvD6yzy8cllE/NeqPj2hm873BE344cqGuIK8u1I/zM4b1i6Ukypa2bpxRFyYwWN08ZY+YaY3oaYwYZYwZhCYSRxpgNwATgKjuKajSw0xizHngDOEtEuohIFywt5Y1ctjPJPOXRyZZGhQuP7ps0as91h/zYVdX87oqRqSsqiqJkmUILHXkNOA9YCuwDvgRgjNkmIncBM+x6d7pyYuWERG3BS3s4blBXS7iENGUVEkXQREVRCpC8Cw1b23C2DXCDT70ngCdaqFmhLP5NS4YWXw+sPg1FUTIh/ysEFShJM8IDBEO+JvcpiqK0NCo0fAgTEeWM1pPqFsFdVbmmKEomFEH3lh+SoqcCzDlBqdEVRVFaEyo0fEg2TyXXccqKc56GoihK+qjQ8CE5eir8Z4thrYYiaKKiKAWICg0fkjvVNMxT2iEritJKUaHhQ3NMTMURPVUMbVQUpdBQoeFLePNUYkryYuiOi0KuKYpScOR9cl+hkigk0vFT+Gkaj35hFHUNuuyPoijFiwoNH8KmRgdr9b74z3rXO+sIr6S/+UEVDUVRMkHNUyHxmrDnCBaTkKW9GEw/xRDhpShK4aFCIySC8Mo3T/I8lqhpFIcjXFEUJX1UaPjgFXF7ZL9OnnXDmqcKiSJooqIoBYgKjZAEaQ+J0VPFoGkUQRMVRSlAVGiExKuPdcoS46G0Q1YUpbWiQiMkgYIg0TxVBMafYmijoiiFhwqNkHiZnJyixOipokhYWARtVBSl8FChEZKgPrYxyRGuPbKiKK0TFRohqCyNcoRH5FTMp5EUcpv7NimKouQDnRGegoFdq3jn+6d5HisviQJek/sKX2oUQRMVRSlAVNPwIWqrCx0q/OVqeal1+4pxnoaiKEomqNDwYUDXKn58wXAev7o6rvyqMQfFtitimkY8xTFPo/DbqChK4aFCI4BrThpMn06VcWV3XnQkt31mONCkaSSqGtodK4rSWlGhkQE19Y0AVJRamkZi9FRRaBr5boCiKEWJCo0MOFDXAEB5iePTKMYst/lugaIoxUjehIaIfFNEFovIfBG5z1V+i4gstY+d7So/xy5bKiI356fVFomahqYRURSlrZCXkFsROQ24CBhhjKkRkZ52+XDgMuAIoC/wpogcYn/s98CZwBpghohMMMYsaPnWQ32DJTSaNI3448Vhnir8NiqKUnjka57G9cC9xpgaAGPMJrv8IuBZu3yFiCwFjrePLTXGLAcQkWftunkRGjecNpSd++u44oSBgIem0fJNSpsikGuKohQg+TJPHQKcLCIfisjbInKcXd4PWO2qt8Yu8ytPQkSuFZGZIjJz8+bNOWg6dK4q475LjqaqzJK5B3WtijteDJqGoihKJuRMaIjImyIyz+PvIiwNpwswGvhf4HmxJg549bYmoDy50JhHjTHVxpjqHj16ZOnbBDN+eC/+8fWxsf1ikBlF0ERFUQqQnJmnjDHj/Y6JyPXAi8YKO5ouIo1AdywNYoCran9gnb3tV14QHDuwS2y7KCbOFUETFUUpPPJlnnoJOB3AdnSXAVuACcBlIlIuIoOBYcB0YAYwTEQGi0gZlrN8Ql5ariiK0obJlyP8CeAJEZkH1AJX21rHfBF5HsvBXQ/cYIxpABCRbwBvAFHgCWPM/Pw0vXWg0VOKomRCXoSGMaYWuNLn2D3APR7lrwGv5bhpbYZisKApilJ46IxwRVEUJTQqNNooqmgoipIJKjTaKEUR4aUoSsGhQkNRFEUJjQqNNorqGYqiZIIKDUVRFCU0KjTaKOrSUBQlE1RotFF0cp+iKJmgQkNRFEUJjQqNtooqGoqiZIAKjTaK+jQURckEFRqKoihKaFRotFFU0VAUJRNUaLRRNI2IoiiZoEJDURRFCY0KjTaK6hmKomSCCo02ilqnFEXJBBUaiqIoSmjytUZ4q+TBy4+lX+fKfDcjFJpGRFGUTFChkUUuPLpvvpsQGjVPKYqSCWqeUhRFUUKjQkNRFEUJjQqNNoqapxRFyQQVGoqiKEpoVGi0UTR6SlGUTFChoSiKooQmL0JDRI4RkWkiMltEZorI8Xa5iMiDIrJUROaIyEjXZ64WkU/sv6vz0e7WhPo0FEXJhHzN07gPuMMY8y8ROc/eHwecCwyz/04A/gCcICJdgduAasAAs0RkgjFmez4a3xpQmaEoSibkyzxlgI72didgnb19EfCUsZgGdBaRPsDZwCRjzDZbUEwCzmnpRiuKorR18qVpfAt4Q0R+iSW4xtrl/YDVrnpr7DK/8iRE5FrgWoCBAwdmt9WtCF1PQ1GUTMiZ0BCRN4HeHoduBc4Avm2MeUFE/gf4IzAeb6uJCShPLjTmUeBRgOrqas86ipqnFEXJjJwJDWPMeL9jIvIUcJO9+zfgcXt7DTDAVbU/lulqDZbPw10+NUtNVRRFUUKSL5/GOuBUe/t04BN7ewJwlR1FNRrYaYxZD7wBnCUiXUSkC3CWXaZkiFqnFEXJhHz5NL4K/EZESoAD2D4I4DXgPGApsA/4EoAxZpuI3AXMsOvdaYzZ1rJNbl2oT0NRlEzIi9AwxrwHjPIoN8ANPp95Angix01TFEVRAtAZ4YqiKEpoVGgoiqIooVGhoSiKooRGhUYbIxpRB7iiKJmja4S3MV678WTe/WRzvpuhKEqRokKjjXFo7w4c2rtDvpuhKEqRouYpRVEUJTQqNBRFUZTQqNBQFEVRQqNCQ1EURQmNCg1FURQlNCo0FEVRlNCo0FAURVFCo0JDURRFCY1Y2chbJyKyGVjVjFN0B7ZkqTnFjt6LePR+xKP3I55ivx8HGWN6eB1o1UKjuYjITGNMdb7bUQjovYhH70c8ej/iac33Q81TiqIoSmhUaCiKoiihUaERzKP5bkABofciHr0f8ej9iKfV3g/1aSiKoiihUU1DURRFCY0KDUVRFCU0KjQ8EJFzRGSxiCwVkZvz3Z6WQEQGiMgUEVkoIvNF5Ca7vKuITBKRT+z/XexyEZEH7Xs0R0RG5vcbZB8RiYrIf0TkFXt/sIh8aN+L50SkzC4vt/eX2scH5bPduUBEOovI30Vkkf2MjGnjz8a37fdknog8IyIVbeX5UKGRgIhEgd8D5wLDgctFZHh+W9Ui1APfNcYcDowGbrC/983AZGPMMGCyvQ/W/Rlm/10L/KHlm5xzbgIWuvZ/Djxg34vtwDV2+TXAdmPMUOABu15r4zfA68aYw4Cjse5Lm3w2RKQfcCNQbYw5EogCl9FWng9jjP65/oAxwBuu/VuAW/Ldrjzch5eBM4HFQB+7rA+w2N5+BLjcVT9WrzX8Af2xOsLTgVcAwZrhW5L4nABvAGPs7RK7nuT7O2TxXnQEViR+pzb8bPQDVgNd7d/7FeDstvJ8qKaRjPNAOKyxy9oMtvp8LPAh0MsYsx7A/t/Trtba79Ovge8DjfZ+N2CHMabe3nd/39i9sI/vtOu3Fg4GNgNP2ua6x0WkHW302TDGrAV+L4UOhwAABFlJREFUCXwKrMf6vWfRRp4PFRrJiEdZm4lLFpH2wAvAt4wxu4KqepS1ivskIhcAm4wxs9zFHlVNiGOtgRJgJPAHY8yxwF6aTFFetOr7YftuLgIGA32BdlgmuURa5fOhQiOZNcAA135/YF2e2tKiiEgplsD4izHmRbt4o4j0sY/3ATbZ5a35Pp0IXCgiK4FnsUxUvwY6i0iJXcf9fWP3wj7eCdjWkg3OMWuANcaYD+39v2MJkbb4bACMB1YYYzYbY+qAF4GxtJHnQ4VGMjOAYXYkRBmWg2tCntuUc0REgD8CC40x97sOTQCutrevxvJ1OOVX2ZEyo4Gdjqmi2DHG3GKM6W+MGYT1+79ljPk8MAW4xK6WeC+ce3SJXb9oR5KJGGM2AKtF5FC76AxgAW3w2bD5FBgtIlX2e+Pcj7bxfOTbqVKIf8B5wBJgGXBrvtvTQt/5JCyVeQ4w2/47D8v2Ohn4xP7f1a4vWFFmy4C5WJEkef8eObgv44BX7O2DgenAUuBvQLldXmHvL7WPH5zvdufgPhwDzLSfj5eALm352QDuABYB84CngfK28nxoGhFFURQlNGqeUhRFUUKjQkNRFEUJjQoNRVEUJTQqNBRFUZTQqNBQFEVRQqNCQ1F8EJEGEZnt+gvMeCwi14nIVVm47koR6Z7B584WkdtFpIuIvNbcdiiKFyWpqyhKm2W/MeaYsJWNMQ/nsjEhOBlrgtkpwL/z3BallaJCQ1HSxE4v8hxwml10hTFmqYjcDuwxxvxSRG4ErsNKOb/AGHOZiHQFnsCaBLYPuNYYM0dEugHPAD2wJn+J61pXYqXhLsNKIPl1Y0xDQnsuxcrGfDBWTqRewC4ROcEYc2Eu7oHSdlHzlKL4U5lgnrrUdWyXMeZ44HdYeakSuRk41hgzAkt4gDWL+D922Q+Bp+zy24D3jJUMcAIwEEBEDgcuBU60NZ4G4POJFzLGPIeVC2qeMeYorFnKx6rAUHKBahqK4k+QeeoZ1/8HPI7PAf4iIi9hpd0AK1XLxQDGmLdEpJuIdMIyJ33WLn9VRLbb9c8ARgEzrBRHVNKUFDCRYVhpOwCqjDG7Q3w/RUkbFRqKkhnGZ9vhfCxhcCHwYxE5guAU2V7nEOBPxphbghoiIjOB7kCJiCwA+ojIbOCbxph3g7+GoqSHmqcUJTMudf3/wH1ARCLAAGPMFKyFnDoD7YF3sM1LIjIO2GKsNUvc5ediJQMEKwngJSLS0z7WVUQOSmyIMaYaeBXLn3EfVpLNY1RgKLlANQ1F8afSHrE7vG6MccJuy0XkQ6yB1+UJn4sCf7ZNT4K1bvQO21H+pIjMwXKEO+my7wCeEZGPgLexUm9jjFkgIj8CJtqCqA64AVjl0daRWA7zrwP3exxXlKygWW4VJU3s6KlqY8yWfLdFUVoaNU8piqIooVFNQ1EURQmNahqKoihKaFRoKIqiKKFRoaEoiqKERoWGoiiKEhoVGoqiKEpo/h+ZymQYe+GiKAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# render a few episodes to watch our trained agent perform the task\n",
    "torender_episodes = 3\n",
    "torender_frames = 300\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "\n",
    "for i in range(torender_episodes):\n",
    "    state = env.reset()\n",
    "    img = plt.imshow(env.render(mode='rgb_array'))\n",
    "    for j in range(torender_frames):\n",
    "        action = agent.act(state)\n",
    "        img.set_data(env.render(mode='rgb_array'))\n",
    "        plt.axis('off')\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            break \n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The End\n",
    "And that's a wrap. I find writing these code blogs extremely useful for my studies in RL and AI in general. I am not a fast learner and much like an agent with Experience Replay, I find it useful to pause lectures and work through implementations and material at my own pace, aiming for a deep level of understanding.  \n",
    "\n",
    "As disclaimed in the repo's readme, the comments and interpretations offered in this notebook are correct only to the extent of my knowledge at the time of writing them. If you find something that I got wrong, do get in touch and you'd be doing me a big favour if you helped me correct it.  \n",
    "\n",
    "If you found this useful in your own studies, I'd love to know. **Say hi to me on [twitter](https://twitter.com/TheFrontalLobe_) or [linkedin](https://www.linkedin.com/in/andre-marques-smith/)**!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
