{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Deep Q-Network Implementation Step by Step\n",
    "\n",
    "## Introduction\n",
    "This notebook is a 'code-blog' entry describing and explaining an implementation of a Deep Q-Network (DQN) as studied in [Udacity's Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).  \n",
    "\n",
    "We will go step by step and explain how and why each class and method is defined, how they work and finally how they all fit together as a workflow for implementing DQNs.  \n",
    "\n",
    "If you are just getting started with DQNs, I suggest you start by checking out this [gitbook chapter](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/deep_q_learning.html). Once you have the basic ideas down, take the time to study Google Deepmind's [landmark paper on solving Atari videomages](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf), as well as the early precursor to it [Riedmiller 2005](http://ml.informatik.uni-freiburg.de/former/_media/publications/rieecml05.pdf).  \n",
    "\n",
    "The particular implementation we are about to study is suitable for environments with continuous observation space and discrete action space, as found in openAI gym's [Box2D](https://gym.openai.com/envs/#box2d) classes of environment. If this terminology sounds like ancient greek to you, read openAI's documentation [here](http://gym.openai.com/docs/).  \n",
    "\n",
    "## Troubleshooting Box2D environment imports\n",
    "I and many others came across the following errors when attempting to import and generate Box2D environments from openAI gym:\n",
    "```Python\n",
    "env=gym.make('LunarLander-v2')\n",
    "AttributeError: module 'gym.envs.box2d' has no attribute 'LunarLander-v2'```\n",
    "```Building wheel for box2d-py (setup.py): finished with status 'error'\n",
    "error: command 'swig.exe' failed: No such file or directory.```  \n",
    "\n",
    "If you haven't, skip ahead. If you have, keep reading and I hope this helps.  \n",
    "\n",
    "I think the **first error** is caused by not having Box2D environments installed within Jupyter Notebook, even if you have installed them in your Anaconda environment. [This blogpost](https://jakevdp.github.io/blog/2017/12/05/installing-python-packages-from-jupyter/) explains difficulties and differences between packages available to your environment vs those available to a Jupyter Notebook you deploy from that environment. The **second error** is a difficulty encountered once you get over the first one. If you succeed in running pip install from your notebook, you may run into the second set of errors if you don't have swig installed on your system, or if its location hasn't been added to System Environment variables.\n",
    "\n",
    "If you run into these issues, the fix that worked for me was the following:\n",
    "1. Use the following line to allow you to run pip installs from Jupyter ([source](https://jakevdp.github.io/blog/2017/12/05/installing-python-packages-from-jupyter/)), and perform a pip install of Box2D environments. This is the last line on the imports block of this notebook;\n",
    "```Python\n",
    "import sys\n",
    "!{sys.executable} -m pip install gym[Box2D]```\n",
    "2. Download swigwin from [this sourceforge link](http://prdownloads.sourceforge.net/swig/swigwin-4.0.1.zip), unzip it and copy the path (for example C:\\swigwin-4.0.1, see [video if helpful](https://www.youtube.com/watch?v=HDD9QqLtAws)). Then add this path to your System Environment variables as shown: <img src=\"path.png\" alt=\"Drawing\" style=\"width: 600px;\"/>  \n",
    "\n",
    "3. Check that swig installed correctly: <img src=\"cmd.png\" alt=\"Drawing\" style=\"width: 400px;\"/>  \n",
    "4. Shut down the notebook, kill current Jupyter processes running in Anaconda with 2x ```Ctrl-C```, launch Anaconda and Jupyter again;\n",
    "5. Run code block 1 (all the imports and pip install) and code block 2:\n",
    "```Python\n",
    "env = gym.make('LunarLander-v2')```\n",
    "and that should instantiate any Box2D environment successfully.\n",
    "\n",
    "That should work; if not, go over [this thread](https://github.com/openai/gym/issues/1603)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym[Box2D] in c:\\users\\andre\\anaconda3\\lib\\site-packages (0.16.0)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from gym[Box2D]) (1.4.10)\n",
      "Requirement already satisfied: six in c:\\users\\andre\\anaconda3\\lib\\site-packages (from gym[Box2D]) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from gym[Box2D]) (1.16.5)\n",
      "Requirement already satisfied: cloudpickle~=1.2.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from gym[Box2D]) (1.2.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\andre\\anaconda3\\lib\\site-packages (from gym[Box2D]) (1.3.1)\n",
      "Requirement already satisfied: box2d-py~=2.3.5; extra == \"box2d\" in c:\\users\\andre\\anaconda3\\lib\\site-packages (from gym[Box2D]) (2.3.8)\n",
      "Requirement already satisfied: future in c:\\users\\andre\\anaconda3\\lib\\site-packages (from pyglet<=1.5.0,>=1.4.0->gym[Box2D]) (0.17.1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "!{sys.executable} -m pip install gym[Box2D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andre\\Anaconda3\\lib\\site-packages\\gym\\logger.py:30: UserWarning: WARN: Box bound precision lowered by casting to float32\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The QNetwork class\n",
    "Create the QNetwork class, which is an FC n-layered network with i) parameters for state size and action size, ii) parameters for number of units in each layer we wish to program, and 3) a method for forward propagation.\n",
    "\n",
    "The line\n",
    "```Python\n",
    "super(QNetwork, self).__init__()```\n",
    "\n",
    "is there because we are going to use QNetwork as a subclass of the parent class Agent. We want it to have both its parent class attributes (from Agent, which we will define shortly), as well as some other attributes which are specific to agents of type QNetwork but not other types of Agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=64, fc2_units=64):\n",
    "        '''Initialise parameters and build the Q-network model.\n",
    "        Arguments\n",
    "        =========\n",
    "            state_size (int): size of state space\n",
    "            action_size (int): size of action space\n",
    "            seed (int): random seed for replicability\n",
    "            '''\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed) # Sets the seed for generating random numbers\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up replay buffer size, minibatch size, discount factor gamma, soft update factor for w tau, learning rate and how often to update the local network. Then check if we have a GPU and cuda available and if so, get it ready to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = int(1e5)\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "tau = 1e-3\n",
    "lr = 5e-4\n",
    "update_every = 4\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Agent class\n",
    "\n",
    "### Agent attributes\n",
    "To endow our Agent with its QNetwork 'brain', we set attributes qnetwork_local and qnetwork_target to be instances of the class QNetwork in the following lines:\n",
    "```Python\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)```\n",
    "Because we're using **Fixed Q Targets**, we actually set up two Q-networks: one which is the target we're trying to approximate and the other the current parameters for the approximation to the Q value function.\n",
    "\n",
    "Here, we initialise the Agent's memory attribute, which is a collection of *experiences*, each of which is a tuple of form (state, action, reward, next_state).\n",
    "```Python\n",
    "        self.memory = ReplayBuffer(action_size, buffer_size, batch_size, seed)```\n",
    "And after it, we add a time-step counter.\n",
    "### Agent methods: step\n",
    "The step function,\n",
    "```Python\n",
    "        def step(self, state, action, reward, next_state, done):```\n",
    "Takes in an experience tuple and adds it to the self.memory attribute of agent, as seen above. Then, it takes one time-step and checks if the number of time-steps after which to update (*update_every*) has been reached:\n",
    "```Python\n",
    "        self.t_step = (self.t_step + 1) % update_every```\n",
    "\n",
    "If it has, *and* enough samples have been stored in memory (threshold defined by *batch_size*), it retrieves an experience tuple at random from memory and then passes it to the .learn method.\n",
    "### Agent methods: act\n",
    "*act* is the method for selecting an epsilon-greedy action given the current policy.  \n",
    "First, we take the state and convert it from numpy to a pytorch tensor, unsqueeze it with 0(*returns a new tensor with a dimension of size one inserted at the specified position*) and send it to the same device where our QNetworks reside.\n",
    "\n",
    "We then set the local QNetwork model to eval() mode. This pytorch method turns off dropout and batch normalisation. The reason for this is that in the next line,\n",
    "```Python\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)```\n",
    "   we are accessing the QNetwork final output values, by passing in a state and computing forward propagation through the QNetwork's current values. Dropout and batch normalisation are used for improving training and generalization, so here we wish to turn them off, in order to select an action. The code that follows is boilerplate for instantiating an epsilon-greedy action.\n",
    "\n",
    "### Agent methods: learn\n",
    "Here we update value parameters given a batch of experience tuples.  \n",
    "First, we *detach* the target QNetwork from the computational graph, ensuring no gradient is backpropped along this network specifically. Then we obtain the maximum predicted Q values for next states from the target model:\n",
    "```Python\n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)```\n",
    "Then, we use the Q-learning formula to compute Q-targets (approximation of true function) for current states that we're learning from. We next obtain the expected Q values from the local model, forward propping *states* through it and gather(ing) the results (think this is similar to zip).  \n",
    "Next, we compute the loss, run backprop and update the local weights:\n",
    "```Python\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()```  \n",
    "Finally, we update the target QNetwork. Notice this is a different update from that executed on the local QNetwork. On the former, we're doing a 'real' learned update with gradient descent. On the latter we're using Fixed Q Targets through the *soft_update* function, which we'll look at next.\n",
    "```Python\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, tau)```\n",
    "### Agent methods: soft update\n",
    "Here the donkey has gotten close enough, so we move the carrot a little bit. The soft update goes by the formula\n",
    "```θ_target = τ*θ_local + (1 - τ)*θ_target```, using tau as initialised at the top. The _ after copy is for making .copy an in-place method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "        # Q-network 'brain'\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
    "        \n",
    "        # Optimizer - notice only used on local qnetwork\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=lr)\n",
    "        \n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, buffer_size, batch_size, seed)\n",
    "        \n",
    "        # Timestep counter\n",
    "        self.t_step = 0\n",
    "        \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save the current experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Learn after every 'update_every' steps\n",
    "        self.t_step = (self.t_step + 1) % update_every\n",
    "        \n",
    "        # if 'update_every' time steps have been reached (modulo division above),\n",
    "        if self.t_step == 0:\n",
    "                # and if there are enough samples in memory:\n",
    "            if len(self.memory) > batch_size:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, gamma)\n",
    "                \n",
    "    def act(self, state, eps=0.):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval() # pytorch method for setting a network to evaluation (stop dropout and batchnorm)\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "        \n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "    \n",
    "    def learn(self, experiences, gamma):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        \n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "        \n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, tau)\n",
    "    \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The ReplayBuffer class\n",
    "Here we define ReplayBuffer which is an object with the ability to carry experiences up to a certain capacity (configured at the top, *buffer_size*).  \n",
    "The attribute of ReplayBuffer which stores experiences is, appropriately, *self.memory*. We package experiences as named tuples in the self.experience attribute:\n",
    "```Python\n",
    "        self.experience = namedtuple(\"Experience\", field_names=['state', 'action', 'reward', 'next_state', 'done'])```\n",
    "Calling self.experience on data of the right format organises that data by returning a tuple of name 'Experience', with the fields defined and named above. \n",
    "\n",
    "### ReplayBuffer methods: add\n",
    ".add takes in arguments for\n",
    "```Python\n",
    "        def add(self, state, action, reward, next_state, done):```\n",
    "and passes them to self.experience for packaging. This experience is then appended to memory, up to the capacity *buffer_size*. Because *memory* is a deque, that means if it reaches capacity, the rule goes \"new in, oldest out\", where we can imagine elements getting added left to right. Once we reach capacity, if a new element is added (at the right), the oldest element falls off at the left end, and everything shifts one step left.\n",
    "\n",
    "### ReplayBuffer methods: sample\n",
    "From *memory* we sample a number *batch_size* of experiences at random, unpack them first into concatenated numpy arrays, converting them to pytorch and then sending them to the device (GPU) where we're doing computation.  \n",
    ".sample returns tensors each containing:\n",
    "```Python\n",
    "        return (states, actions, rewards, next_states, dones)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=['state', 'action', 'reward', 'next_state', 'done'])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory, k = self.batch_size)\n",
    "        \n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Finally, we will build a function for controlling the agent-environment interaction, calling the functions and classes we have defined above appropriately. The algorithm created in this notebook can be used with environments that have continuous observation space and discrete action space.\n",
    "\n",
    "### DQN function\n",
    "We start out by defining a list of running scores and a deque (special list-like container with a max capacity then last-in-first-out bevaiour) to keep a windowed version of 100 scores, as well as initialising epsilon at a specified value.  \n",
    "We then run a for loop for a given number of episodes, and within this we nest another for loop that defines a maximum length of timesteps for each episode. Let's break down the contents of this loop:\n",
    "```Python\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break```\n",
    "    1. agent.act computes the action values for a given input state and returns the epsilon-greedy action;\n",
    "    2. Given the choice of action, env.step triggers a timestep in the environment, returning a new state, reward and a 'done' boolean.\n",
    "    3. agent.step does a number of important things:\n",
    "        3.1. First, it add's to the agent's memory the current state, action, reward, next_state and done variables;\n",
    "        3.2. Then it updates the agent's timestep counter and checks if it's a multiple of *update_every*, the frequency of episodes over which we want the agent to do replay and offline learning;\n",
    "        3.3. If that frequency has been reached *and* there are a certain number of experiences in the agent's memory, it calls the method sample() from the ReplayBuffer class to retrieve a *batch_size* number of experiences from memory.\n",
    "        3.4. Finally it calls the Agent function *learn*, which does the following:\n",
    "            3.4.1. Unpack each experience tuple into states, actions, rewards, next_state and one;\n",
    "            3.4.2. Perform Q-learning  by obtaining the max Q value for the next state from the target model, compute the Q targets given the current state, and retrieve expected Q values given the current local model;\n",
    "            3.4.3. Calculate loss, run backprop and update weights for the Local Q Network;\n",
    "            3.4.4. Calls *soft_update* function to update Q in the Target Network, moving the target weights away from the current ones by a factor of tau.\n",
    "\n",
    "\n",
    "At the end of an episode, we append that episode's score (running tally of reward) to scores and score_window and print the average of scores_window, which gives us the running average over the last 100 episodes. If we achieve the solve score, it prints this out and saves the model checkpoint. Finally, the function returns *scores*, the episode-by-episode running tally of scores attained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn(n_episodes = 2000, max_t=1000, eps_start=1.0, eps_decay=0.995, eps_end=0.01, solve_score=200.0):\n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    eps = eps_start\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        \n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "    \n",
    "        scores_window.append(score)\n",
    "        scores.append(score)\n",
    "        eps = max(eps_end, eps_end*eps)\n",
    "\n",
    "        print('\\rEpisode {}\\tAverage Score {:.2f}'.format(i_episode, np.mean(scores_window)), end='')\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window)>=solve_score:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together\n",
    "Before beginning to train our agent, let's take a birds-eye schematic view of how the parts of the whole implementation fit together. <img src=\"dqn.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape:  (8,)\n",
      "Number of actions:  4\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-3bf9b12e24b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Agent' is not defined"
     ]
    }
   ],
   "source": [
    "env.seed(0)\n",
    "print('State shape: ', env.observation_space.shape)\n",
    "s = env.observation_space.shape\n",
    "print('Number of actions: ', env.action_space.n)\n",
    "a = env.action_space.n\n",
    "\n",
    "agent = Agent(state_size=s, action_size=a, seed=0)\n",
    "scores = dqn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize\n",
    "Training is now completed. Hopefully the agent managed to solve the task, or at least learn something useful. In the next two codeblocks we plot the agent's score per episode and then watch our trained agent perform a few trials. Change how many episodes you wish to visualise and how many frames per episode to render by toggling the variables at the top of the last codeblock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# render a few episodes to watch our trained agent perform the task\n",
    "torender_episodes = 3\n",
    "torender_frames = 300\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "\n",
    "for i in range(torender_episodes):\n",
    "    state = env.reset()\n",
    "    img = plt.imshow(env.render(mode='rgb_array'))\n",
    "    for j in range(torender_frames):\n",
    "        action = agent.act(state)\n",
    "        img.set_data(env.render(mode='rgb_array'))\n",
    "        plt.axis('off')\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            break \n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
