{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 80 * 80 # input dimensionality: 80x80 grid\n",
    "h = 200\n",
    "o = 1\n",
    "\n",
    "batch_size = 10 # every how many episodes to do a param update?\n",
    "learning_rate = 1e-4\n",
    "gamma = 0.99 # discount factor for reward\n",
    "resume = False # resume from previous checkpoint?\n",
    "render = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(PolicyNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x#.cpu().data\n",
    "    \n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        p = self.forward(state)\n",
    "        action = 4 if np.random.uniform() < p else 5 \n",
    "        return action, torch.log(p)\n",
    "\n",
    "def preprocess_state(state):\n",
    "    img = state[35:195,:,:]\n",
    "    img = img[::2, ::2, 0]\n",
    "    img[img==144] = 0\n",
    "    img[img==109] = 0\n",
    "    img[img!=0] = 1\n",
    "    return img.astype(np.float).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch GD\n",
    "_________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_pong_minibatch(n_batches=30, gamma=0.9, print_every=100, render=False, b=32):\n",
    "    scores_deque = deque(maxlen=b)\n",
    "    scores = []\n",
    "    \n",
    "    bn=0\n",
    "    while bn < n_batches:\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        batch_saved_log_probs = []\n",
    "        batch_rewards = []\n",
    "        for i_episode in range(bn*b, (bn+1)*b):\n",
    "            ep_rewards = []\n",
    "            observation = env.reset()\n",
    "            prev_input = None\n",
    "            \n",
    "            t=0####\n",
    "            while True:\n",
    "                if render:\n",
    "                    env.render()\n",
    "                cur_input = preprocess_state(observation)\n",
    "                state = cur_input - prev_input if prev_input is not None else cur_input\n",
    "                prev_input = cur_input\n",
    "\n",
    "                action, log_prob = agent.act(state)\n",
    "                saved_log_probs.append(log_prob)\n",
    "\n",
    "                observation, reward, done, _ = env.step(action)\n",
    "                rewards.append(reward*gamma**t)   ####\n",
    "                ep_rewards.append(reward)\n",
    "                t+=1    ###\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            scores_deque.append(sum(ep_rewards))\n",
    "            scores.append(sum(ep_rewards))\n",
    "            batch_saved_log_probs.append(sum(saved_log_probs))\n",
    "            batch_rewards.append(sum(rewards))\n",
    "\n",
    "        #discounts = [gamma**i for i in range(len(rewards)+1)]\n",
    "        #R = sum([a*b for a,b in zip(discounts, rewards)])/b\n",
    "        #nR = [a*b for a,b in zip(discounts, rewards)]\n",
    "        #nR = (nR - np.mean(nR))/np.std(nR)\n",
    "\n",
    "        policy_loss = []\n",
    "        for l, r in zip(batch_saved_log_probs, batch_rewards):\n",
    "            policy_loss.append(-l * r)\n",
    "        #for log_prob in saved_log_probs:\n",
    "        #    policy_loss.append(-log_prob * nR)\n",
    "        policy_loss = torch.cat(policy_loss).sum()/b\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print('Batch #{}/{}:\\tAverage Score {:.2f}\\tLoss {:.2f}'.format(bn+1,\n",
    "                                                                        n_batches,\n",
    "                                                                        np.mean(scores_deque),\n",
    "                                                                       policy_loss))\n",
    "        \n",
    "        bn += 1\n",
    "            \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda:0.\n",
      "Batch #1/30:\tAverage Score -19.56\tLoss -206758.22\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Training on {}.'.format(device))\n",
    "\n",
    "agent = PolicyNN(i,h,o).to(device)\n",
    "env = gym.make(\"PongDeterministic-v4\")\n",
    "optimizer = optim.Adam(agent.parameters(), lr=learning_rate)\n",
    "\n",
    "scores = play_pong_minibatch(render=False, gamma=0.995, b=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda:0.\n"
     ]
    }
   ],
   "source": [
    "### step by step debug\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Training on {}.'.format(device))\n",
    "\n",
    "agent = PolicyNN(i,h,o).to(device)\n",
    "env = gym.make(\"PongDeterministic-v4\")\n",
    "optimizer = optim.Adam(agent.parameters(), lr=learning_rate)\n",
    "scores_deque = deque(maxlen=100)\n",
    "scores = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### batch loop\n",
    "saved_log_probs = []\n",
    "rewards = []\n",
    "batch_saved_log_probs = []\n",
    "batch_rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "### episode loop starts here\n",
    "ep_rewards = []\n",
    "observation = env.reset()\n",
    "prev_input = None\n",
    "l = len(saved_log_probs)\n",
    "t = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "764\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    if render:\n",
    "        env.render()\n",
    "    cur_input = preprocess_state(observation)\n",
    "    state = cur_input - prev_input if prev_input is not None else cur_input\n",
    "    prev_input = cur_input\n",
    "\n",
    "    action, log_prob = agent.act(state)\n",
    "    saved_log_probs.append(log_prob)\n",
    "\n",
    "    observation, reward, done, _ = env.step(action)\n",
    "    rewards.append(reward*0.99**t)\n",
    "    ep_rewards.append(reward)\n",
    "    t+=1\n",
    "    if done:\n",
    "        print(t)\n",
    "        break\n",
    "\n",
    "scores_deque.append(sum(ep_rewards))\n",
    "scores.append(sum(ep_rewards))\n",
    "batch_saved_log_probs.append(sum(saved_log_probs))\n",
    "batch_rewards.append(sum(rewards))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We played 3 matches with scores [-19.0, -20.0, -21.0].\n",
      "L of log probs is 2641, increase of 764.\n",
      "3 3 [tensor([[-673.6816]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[-1281.4662]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[-1803.2837]], device='cuda:0', grad_fn=<AddBackward0>)] [-1.7123383686895763, -3.494628757364642, -5.283779504893986]\n"
     ]
    }
   ],
   "source": [
    "print('We played {} matches with scores {}.'.format(len(scores), scores))\n",
    "[i for i in rewards if i>0]\n",
    "\n",
    "print('L of log probs is {}, increase of {}.'.format(len(saved_log_probs), len(saved_log_probs)-l))\n",
    "print(len(batch_saved_log_probs), len(batch_rewards), batch_saved_log_probs, batch_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-1153.5708]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       " tensor([[-4478.2485]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       " tensor([[-9528.1533]], device='cuda:0', grad_fn=<MulBackward0>)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### loop here\n",
    "#discounts = [gamma**i for i in range(len(rewards)+1)]\n",
    "#R = [a*b for a,b in zip(discounts, rewards)]\n",
    "#nR = [a*b for a,b in zip(discounts, rewards)]\n",
    "#nR = (nR - np.mean(nR))/np.std(nR)\n",
    "#print(len(discounts))\n",
    "#print('Discounted R total {:.3f}, avg score {:.1f}'.format(R, np.mean(scores_deque)))\n",
    "policy_loss = []\n",
    "for l, r in zip(batch_saved_log_probs, batch_rewards):\n",
    "    policy_loss.append(-l*r)\n",
    "policy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'backward'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-3590d6de2958>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicy_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpolicy_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'backward'"
     ]
    }
   ],
   "source": [
    "torch.cat(policy_loss).sum()\n",
    "policy_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Training on {}.'.format(device))\n",
    "\n",
    "agent = PolicyNN(i,h,o).to(device)\n",
    "env = gym.make(\"PongDeterministic-v4\")\n",
    "optimizer = optim.Adam(agent.parameters(), lr=learning_rate)\n",
    "\n",
    "scores = play_pong_minibatch(render=False, gamma=0.97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "32*30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
