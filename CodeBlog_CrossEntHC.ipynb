{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Entropy Hill-Climbing with a 2-Layer Neural Network\n",
    "Here we will train an agent with a 2-layer neural network to solve a continuous state and action task, [MountainCar](https://gym.openai.com/envs/MountainCarContinuous-v0/), using the [cross-entropy method](https://en.wikipedia.org/wiki/Cross-entropy_method). I've previously [implemented CEM](https://github.com/andrefmsmith/amsRL_openAIgym/blob/master/CodeBlog_Cartpole_Policy.ipynb) on a simpler task, Cartpole, solvable with a single-layer network.  \n",
    "\n",
    "In CEM, when applied to policy networks in reinforcement learning, at every episode we generate a range of candidate policies (weights and biases) by adding noise, drawn from a gaussian distribution, to our current set of policy parameters. We then evaluate the candidate policies by collecting the return on the episode generated by each and averaging the parameters of the top performing policies. Optionally, we can use adaptive noise scaling to change the magnitude of our noise parameter (how much to scale the gaussian by) at each iteration, depending on whether compared to the previous iteration return was increased (*decrease* noise, exploring the neighbourhood with smaller steps) or decreased (*increase* noise, exploring a broader neighbourhood of parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment\n",
    "```MountainCarContinuous-v0``` has **continual** observation and action spaces, the former being two-dimensional and the latter one-dimensional. The agent controls a car attempting to climb over a steep hill. However, its 'engine' isn't powerful enough to scale the mountain in a single pass:  \n",
    "<img src=\"https://pythonawesome.com/content/images/2019/07/mountaincar.gif\" width=\"400\">  \n",
    "\n",
    "Therefore, the agent has to gather enough momentum by driving back and forth, controlling acceleration appropriately in order to gather enough momentum to scale over the mountain. The action is a single continuous value between -1 and 1. We will use a neural network with one hidden layer to learn a policy to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space: Box(2,)\n",
      "action space: Box(1,)\n",
      "  - low: [-1.]\n",
      "  - high: [1.]\n",
      "Training on cuda:0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\andre\\desktop\\udacity drl\\gym\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "print('observation space:', env.observation_space)\n",
    "print('action space:', env.action_space)\n",
    "print('  - low:', env.action_space.low)\n",
    "print('  - high:', env.action_space.high)\n",
    "print('Training on {}.'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy network Agent and its methods\n",
    "We first define an Agent class inheriting properties from a pyTorch neural network class. First we configure the architecture of the **neural network** which will have our policy embedded:\n",
    "```Python\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, env, h_size=16):\n",
    "        super(Agent, self).__init__()```\n",
    "The input layer size matches the dimensionality of our observation space and the output layer size that of the action space, with a hiden layer of size ```h_size``` in between:\n",
    "```Python\n",
    "        self.fc1 = nn.Linear(self.s_size, self.h_size)\n",
    "        self.fc2 = nn.Linear(self.h_size, self.a_size)```\n",
    "  \n",
    "We define a **forward propagation** method where we use ReLu on the hidden layer and tanh on the output, since we want output to vary continuously in [-1, 1]. \n",
    "```Python\n",
    "    def forward(self,x ):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))#F.tanh(self.fc2(x))\n",
    "        return x.cpu().data```\n",
    "\n",
    "The **populate** method:\n",
    "```Python\n",
    "def populate(self, best_wb, noise):```\n",
    "takes in an array of weights and biases ```best_wb```, which is expected to be structured as an array of lists whereby ```best_wb[0]``` corresponds to the weights of the hidden layer and ```best_wb[1]``` to the weights of the output layer. Indices 2 and 3 correspond to the respective layer's biases. If we wanted to use a different architecture, ```populate``` should be changed and its unpacking of weights and biases edited to respect this organisational scheme.  \n",
    "\n",
    "For each set of parameters unpacked, ```populate``` adds to it an array of values of the same size, drawn from a gaussian distribution scaled by a noise parameter, then returns a new array of parameters organised in the same way as its input ```best_wb```.  \n",
    "\n",
    "We then define a method for **assigning weights** to the right layers of our policy network.\n",
    "```Python\n",
    "def assign_weights(self, new_weights):```\n",
    "We will need to call this method at each iteration in order to update our policy network's parameters. For each layer, we call the right index from the weights matrix and turn it into a torch float tensor then copy in-place onto the policy network's parameters.  \n",
    "\n",
    "Finally, for convenience and tidiness we define an **evaluate** method:\n",
    "```Python\n",
    "    def evaluate(self, weights, gamma=1.0, max_t=5000):\n",
    "        self.assign_weights(weights)\n",
    "        episode_return = 0.0\n",
    "        state = self.env.reset()\n",
    "        for t in range(max_t):```\n",
    "which begins by assigning to our policy network a new set of parameters, passing them to the network for forward propagation to compute which action to take and tally up the return generated by that action. This method returns the total episode return for a given set of input weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, env, h_size=16):\n",
    "        super(Agent, self).__init__()\n",
    "        self.env = env\n",
    "        # state, hidden layer, action sizes\n",
    "        self.s_size = env.observation_space.shape[0]\n",
    "        self.h_size = h_size\n",
    "        self.a_size = env.action_space.shape[0]\n",
    "        # define layers\n",
    "        self.fc1 = nn.Linear(self.s_size, self.h_size)\n",
    "        self.fc2 = nn.Linear(self.h_size, self.a_size)\n",
    "    \n",
    "    def forward(self,x ):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))#F.tanh(self.fc2(x))\n",
    "        return x.cpu().data\n",
    "        \n",
    "    def populate(self, best_wb, noise):\n",
    "        w_fc1 = best_wb[0] + noise*(np.random.randn(self.h_size, self.s_size))#noise*(np.random.rand(self.s_size, self.h_size))\n",
    "        w_fc2 = best_wb[1] + noise*(np.random.randn(self.a_size, self.h_size))#noise*(np.random.rand(self.h_size, self.a_size))\n",
    "        \n",
    "        b_fc1 = best_wb[2] + noise*np.random.randn(self.h_size)\n",
    "        b_fc2 = best_wb[3] + noise*np.random.randn(self.a_size)\n",
    "        return np.array((w_fc1, w_fc2, b_fc1, b_fc2))\n",
    "    \n",
    "    def assign_weights(self, new_weights):\n",
    "        self.fc1.weight.data.copy_(torch.from_numpy(new_weights[0].astype(float)))\n",
    "        self.fc2.weight.data.copy_(torch.from_numpy(new_weights[1].astype(float)))\n",
    "        self.fc1.bias.data.copy_(torch.from_numpy(new_weights[2].astype(float)))\n",
    "        self.fc2.bias.data.copy_(torch.from_numpy(new_weights[3].astype(float)))\n",
    "        \n",
    "    def evaluate(self, weights, gamma=1.0, max_t=5000):\n",
    "        self.assign_weights(weights)\n",
    "        episode_return = 0.0\n",
    "        state = self.env.reset()\n",
    "        for t in range(max_t):\n",
    "            state = torch.from_numpy(state).float().to(device)\n",
    "            action = self.forward(state)\n",
    "            state, reward, done, _ = self.env.step(action)\n",
    "            episode_return += reward * math.pow(gamma, t)\n",
    "            if done:\n",
    "                break\n",
    "        return episode_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "We can now define a training function, ```cem```. The best weight is initialised using ```agent.populate``` inputting a vector of zeros as the initial network, to which an appropriately-sized gaussian is added (noise=1). For each iteration, we draw up a list of candidate policies, each of them itself a list of numpy arrays, then compute the return for each policy and identify the ```topk``` policies. A new ```best_weight``` is computed by averaging parameters over these policies, its return is computed by forward propagation (using ```evaluate``` method) and added to the tally of scores. If the latest score represents an improvement over the previous one, we lower the noise parameter; else we increase it (adaptive noise scaling). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cem(n_iterations=500, max_t=1000, gamma=1.0, print_every=10, pop_size=50, topk=10, noise=0.5):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    best_weight = agent.populate([0,0,0,0], noise=1)\n",
    "    \n",
    "    for i_iteration in range(1, n_iterations+1):\n",
    "        weights_pop = [agent.populate(best_weight, noise) for i in range(pop_size)]\n",
    "        returns = np.array([agent.evaluate(weights, gamma, max_t) for weights in weights_pop])\n",
    "        \n",
    "        elite_idxs = returns.argsort()[-topk:]\n",
    "        elite_weights = [weights_pop[i] for i in elite_idxs]\n",
    "        best_weight = np.array(elite_weights).mean(axis=0)\n",
    "        \n",
    "        return_ = agent.evaluate(best_weight, gamma=1.0)\n",
    "        scores_deque.append(return_)\n",
    "        scores.append(return_)\n",
    "        if return_ >= scores[-1]:\n",
    "            noise*=0.9\n",
    "        else:\n",
    "            noise*=2\n",
    "        \n",
    "        if i_iteration % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_iteration, np.mean(scores_deque)))\n",
    "\n",
    "        if np.mean(scores_deque)>=90.0:\n",
    "            print('\\nEnvironment solved in {:d} iterations!\\tAverage Score: {:.2f}'.format(i_iteration-100, np.mean(scores_deque)))\n",
    "            break\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_size = 2\n",
    "h_size = 16\n",
    "a_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10\tAverage Score: -1.69\n",
      "Episode 20\tAverage Score: -1.34\n",
      "Episode 30\tAverage Score: -1.11\n"
     ]
    }
   ],
   "source": [
    "agent=Agent(env).to(device)\n",
    "scores = cem(pop_size=40, topk=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
