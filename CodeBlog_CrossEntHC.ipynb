{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space: Box(2,)\n",
      "action space: Box(1,)\n",
      "  - low: [-1.]\n",
      "  - high: [1.]\n",
      "Training on cpu.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andre/Desktop/Deep_RL/gym/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "print('observation space:', env.observation_space)\n",
    "print('action space:', env.action_space)\n",
    "print('  - low:', env.action_space.low)\n",
    "print('  - high:', env.action_space.high)\n",
    "print('Training on {}.'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, env, h_size=16):\n",
    "        super(Agent, self).__init__()\n",
    "        self.env = env\n",
    "        # state, hidden layer, action sizes\n",
    "        self.s_size = env.observation_space.shape[0]\n",
    "        self.h_size = h_size\n",
    "        self.a_size = env.action_space.shape[0]\n",
    "        # define layers\n",
    "        self.fc1 = nn.Linear(self.s_size, self.h_size)\n",
    "        self.fc2 = nn.Linear(self.h_size, self.a_size)\n",
    "    \n",
    "    def forward(self,x ):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        return x.cpu().data\n",
    "        \n",
    "    def populate(self, best_wb, noise=1e-3):\n",
    "        w_fc1 = best_wb[0] + noise*(np.random.rand(self.h_size, self.s_size))#noise*(np.random.rand(self.s_size, self.h_size))\n",
    "        w_fc2 = best_wb[1] + noise*(np.random.rand(self.a_size, self.h_size))#noise*(np.random.rand(self.h_size, self.a_size))\n",
    "        \n",
    "        b_fc1 = best_wb[2] + noise*np.random.rand(self.h_size)\n",
    "        b_fc2 = best_wb[3] + noise*np.random.rand(self.a_size)\n",
    "        return np.array((w_fc1, w_fc2, b_fc1, b_fc2))\n",
    "    \n",
    "    def assign_weights(self, new_weights):\n",
    "        self.fc1.weight.data.copy_(torch.from_numpy(new_weights[0].astype(float)))\n",
    "        self.fc2.weight.data.copy_(torch.from_numpy(new_weights[1].astype(float)))\n",
    "        self.fc1.bias.data.copy_(torch.from_numpy(new_weights[2].astype(float)))\n",
    "        self.fc2.bias.data.copy_(torch.from_numpy(new_weights[3].astype(float)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(n_episodes=500, max_t=1000, gamma=1.0, print_every=10, pop_size=10, top_k=3, noise=1e-3):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    best_wb = agent.populate([0,0,0,0], noise=1e-5)\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        episode_rewards = np.zeros((max_t, pop_size))\n",
    "        candidate_policies = [agent.populate(best_wb, noise) for i in range(pop_size)]\n",
    "        state_alpha = env.reset()\n",
    "        \n",
    "        for p in range(pop_size):\n",
    "            state = state_alpha\n",
    "            agent.assign_weights(candidate_policies[p])\n",
    "        \n",
    "            for t in range(max_t):\n",
    "                action = agent.forward(torch.from_numpy(state).float())\n",
    "                state, reward, done, _ = env.step(action)\n",
    "                episode_rewards[t,p] = reward\n",
    "                if done:\n",
    "                    env.reset()\n",
    "                    break\n",
    "                    \n",
    "        discounts = np.array([gamma**i for i in range(max_t)])\n",
    "        returns = np.sum(np.array([a*b for a,b in zip(episode_rewards, discounts)]), axis = 0)\n",
    "        \n",
    "        best_p = np.argmax(returns)\n",
    "        top_ps = heapq.nlargest( top_k , range(len(returns)), returns.__getitem__)\n",
    "        \n",
    "        top_policies = [candidate_ps[i] for i in top_ps]\n",
    "        best_parameters = np.mean(top_policies, axis=0)\n",
    "        \n",
    "        agent.assign_weights(best_parameters)\n",
    "        \n",
    "        scores_deque.append(np.sum(episode_rewards[:,best_p]))\n",
    "        scores.append(np.sum(episode_rewards[:,best_p]))\n",
    "        \n",
    "        if i_episode % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "        if np.mean(scores_deque) >= 90.0:\n",
    "            if len(scores_deque) == 100:\n",
    "                print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-99, np.mean(scores_deque)))\n",
    "                agent.assign_weights(best_parameters)\n",
    "                break\n",
    "            \n",
    "    return scores, returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.2843, -0.3614],\n",
      "        [ 0.3059,  0.4609],\n",
      "        [-0.2916,  0.1454],\n",
      "        [-0.1825,  0.6808],\n",
      "        [-0.1339,  0.4716],\n",
      "        [ 0.6329,  0.1993],\n",
      "        [ 0.4111, -0.6844],\n",
      "        [ 0.5990,  0.1772],\n",
      "        [-0.4535, -0.2630],\n",
      "        [-0.2680, -0.2584],\n",
      "        [-0.5455, -0.3634],\n",
      "        [-0.0699,  0.1330],\n",
      "        [-0.2852,  0.5207],\n",
      "        [-0.5103, -0.4414],\n",
      "        [-0.0993, -0.6163],\n",
      "        [-0.0735,  0.0413]])\n",
      "Parameter containing:\n",
      "tensor([-0.2862, -0.0851, -0.1273, -0.4504, -0.1355, -0.6108, -0.5397,\n",
      "         0.1756,  0.6461, -0.0182,  0.1546,  0.3281,  0.1728, -0.6930,\n",
      "        -0.2634, -0.0949])\n",
      "Parameter containing:\n",
      "tensor([[-0.1489, -0.0494,  0.2436, -0.0175,  0.0619, -0.2255,  0.1341,\n",
      "          0.0889, -0.1292,  0.0803,  0.0871, -0.1735,  0.2345, -0.1314,\n",
      "         -0.2450,  0.1375]])\n",
      "Parameter containing:\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.6531])\n",
      "Episode 10\tAverage Score: -2.35\n",
      "Episode 20\tAverage Score: -2.42\n"
     ]
    }
   ],
   "source": [
    "s_size = 2\n",
    "h_size = 16\n",
    "a_size = 1\n",
    "\n",
    "agent=Agent(env).to(device)\n",
    "scores, returns = cross_entropy(noise=1e-1, top_k=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
